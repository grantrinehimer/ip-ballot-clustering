{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 1 - Sum of Std Dev vs. Centroid Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ELECTION_DIR = 'scot-elex-main'\n",
    "CLUSTERINGS = '2_and_3_clusterings.pkl'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from Clustering_Functions import Borda_vector, HH_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 7 cand elections\n",
    "clusterings = pd.read_pickle(CLUSTERINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_clusterings = clusterings[clusterings.num_clusters == 2]\n",
    "methods = ['continuous', 'continuous_rest', 'discrete']\n",
    "proxies = ['BP', 'BA', 'HH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_to_proxy(clustering, proxy, num_cands):\n",
    "    proxy_clustering = {}\n",
    "    for c_index, assignment in clustering.items():\n",
    "        proxy_assignment = {}\n",
    "        for ballot, weight in assignment.items():\n",
    "            if proxy == 'BP':\n",
    "                ballot_proxy = Borda_vector(ballot, num_cands=num_cands, borda_style='pes')\n",
    "            elif proxy == 'BA':\n",
    "                ballot_proxy = Borda_vector(ballot, num_cands=num_cands, borda_style='avg')\n",
    "            elif proxy == 'HH':\n",
    "                ballot_proxy = HH_proxy(ballot, num_cands=num_cands)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid proxy type\")\n",
    "            proxy_assignment[tuple(ballot_proxy)] = proxy_assignment.setdefault(tuple(ballot_proxy), 0) + weight \n",
    "        proxy_clustering[c_index] = proxy_assignment\n",
    "    return proxy_clustering\n",
    "\n",
    "def cluster_stdev(centroid, points):\n",
    "    # Extract L1 distances and corresponding weights\n",
    "    centroid = np.array(centroid)\n",
    "    distances = []\n",
    "    weights = []\n",
    "    for point, weight in points.items():\n",
    "        point = np.array(point)\n",
    "        l1_dist = np.sum(np.abs(point - centroid))\n",
    "        distances.append(l1_dist)\n",
    "        weights.append(weight)\n",
    "    \n",
    "    # Convert to numpy arrays for vectorized operations\n",
    "    distances = np.array(distances)\n",
    "    weights = np.array(weights)\n",
    "    \n",
    "    sum_weights = np.sum(weights)\n",
    "    if sum_weights == 0:\n",
    "        raise ValueError(\"Sum of weights cannot be zero.\")\n",
    "    \n",
    "    # Calculate weighted mean of distances\n",
    "    weighted_mean = np.sum(distances * weights) / sum_weights\n",
    "    \n",
    "    # Compute weighted variance\n",
    "    squared_diffs = (distances - weighted_mean) ** 2\n",
    "    weighted_variance = np.sum(weights * squared_diffs) / sum_weights\n",
    "    \n",
    "    # Standard deviation is the square root of variance\n",
    "    return np.sqrt(weighted_variance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_1_SAVE = '7CandidateElectionPlots/stdev_v_distance'\n",
    "plot_1_data = pd.DataFrame(columns=['filename', 'method', 'proxy_type', 'stdev', 'distance'])\n",
    "for method in methods:\n",
    "    for proxy in proxies:\n",
    "        elections = two_clusterings[(two_clusterings.method == method) & (two_clusterings.proxy_type == proxy)]\n",
    "        election_names = []\n",
    "        stdevs = [] \n",
    "        distances = []\n",
    "\n",
    "        for i, election in elections.iterrows():\n",
    "            clustering = election.clustering\n",
    "            centroids = election.proxies_of_centers\n",
    "            num_cands = election.num_cands\n",
    "            proxy_clustering = clustering_to_proxy(clustering, proxy, num_cands=num_cands)\n",
    "            stdev = cluster_stdev(centroids[0], proxy_clustering[0]) + cluster_stdev(centroids[1], proxy_clustering[1])\n",
    "            stdevs.append(stdev)\n",
    "            distance = np.sum(np.abs(np.array(centroids[0]) - np.array(centroids[1])))\n",
    "            distances.append(distance)\n",
    "            # Append to plot_1_data\n",
    "            plot_1_data.loc[i] = {\n",
    "                'filename': election['filename'],\n",
    "                'method': method,\n",
    "                'proxy_type': proxy,\n",
    "                'stdev': stdev,\n",
    "                'distance': distance\n",
    "            }\n",
    "\n",
    "\n",
    "        plt.scatter(distances, stdevs)\n",
    "        plt.title(f\"Standard Deviation vs Distance for {method} {proxy}\")\n",
    "        plt.xlabel(\"Distance between centroids\")\n",
    "        plt.ylabel(\"Sum of standard deviations\")\n",
    "        plt.savefig(f\"{PLOT_1_SAVE}/{method}_{proxy}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>method</th>\n",
       "      <th>proxy_type</th>\n",
       "      <th>stdev</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>edinburgh_2017_ward6.csv</td>\n",
       "      <td>discrete</td>\n",
       "      <td>HH</td>\n",
       "      <td>4.385784</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>edinburgh_2022_ward10.csv</td>\n",
       "      <td>discrete</td>\n",
       "      <td>HH</td>\n",
       "      <td>4.081137</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename    method proxy_type     stdev  distance\n",
       "1215   edinburgh_2017_ward6.csv  discrete         HH  4.385784      23.0\n",
       "1216  edinburgh_2022_ward10.csv  discrete         HH  4.081137      23.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_1_data.query(\"4.0 < stdev < 4.4 and 20 < distance < 24 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>method</th>\n",
       "      <th>proxy_type</th>\n",
       "      <th>stdev</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>edinburgh_2017_ward6.csv</td>\n",
       "      <td>continuous</td>\n",
       "      <td>BP</td>\n",
       "      <td>8.249481</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>edinburgh_2022_ward10.csv</td>\n",
       "      <td>continuous</td>\n",
       "      <td>BP</td>\n",
       "      <td>7.229138</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>edinburgh_2017_ward6.csv</td>\n",
       "      <td>continuous_rest</td>\n",
       "      <td>BP</td>\n",
       "      <td>8.214180</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>edinburgh_2022_ward10.csv</td>\n",
       "      <td>continuous_rest</td>\n",
       "      <td>BP</td>\n",
       "      <td>7.786050</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>edinburgh_2017_ward6.csv</td>\n",
       "      <td>discrete</td>\n",
       "      <td>BP</td>\n",
       "      <td>8.214180</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>edinburgh_2022_ward10.csv</td>\n",
       "      <td>discrete</td>\n",
       "      <td>BP</td>\n",
       "      <td>7.786050</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>edinburgh_2017_ward6.csv</td>\n",
       "      <td>discrete</td>\n",
       "      <td>BA</td>\n",
       "      <td>6.397317</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>edinburgh_2022_ward10.csv</td>\n",
       "      <td>discrete</td>\n",
       "      <td>BA</td>\n",
       "      <td>6.247131</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>edinburgh_2017_ward6.csv</td>\n",
       "      <td>discrete</td>\n",
       "      <td>HH</td>\n",
       "      <td>4.385784</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>edinburgh_2022_ward10.csv</td>\n",
       "      <td>discrete</td>\n",
       "      <td>HH</td>\n",
       "      <td>4.081137</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename           method proxy_type     stdev  \\\n",
       "87     edinburgh_2017_ward6.csv       continuous         BP  8.249481   \n",
       "88    edinburgh_2022_ward10.csv       continuous         BP  7.229138   \n",
       "369    edinburgh_2017_ward6.csv  continuous_rest         BP  8.214180   \n",
       "370   edinburgh_2022_ward10.csv  continuous_rest         BP  7.786050   \n",
       "933    edinburgh_2017_ward6.csv         discrete         BP  8.214180   \n",
       "934   edinburgh_2022_ward10.csv         discrete         BP  7.786050   \n",
       "651    edinburgh_2017_ward6.csv         discrete         BA  6.397317   \n",
       "652   edinburgh_2022_ward10.csv         discrete         BA  6.247131   \n",
       "1215   edinburgh_2017_ward6.csv         discrete         HH  4.385784   \n",
       "1216  edinburgh_2022_ward10.csv         discrete         HH  4.081137   \n",
       "\n",
       "      distance  \n",
       "87        20.0  \n",
       "88        17.0  \n",
       "369       21.0  \n",
       "370       19.0  \n",
       "933       21.0  \n",
       "934       19.0  \n",
       "651       14.0  \n",
       "652       16.0  \n",
       "1215      23.0  \n",
       "1216      23.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_1_data.query(\"filename == 'edinburgh_2022_ward10.csv' or filename == 'edinburgh_2017_ward6.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_1_SAVE = 'plots/stdev_v_distance_by_cluster'\n",
    "for method in methods:\n",
    "    for proxy in proxies:\n",
    "        elections = two_clusterings[(two_clusterings.method == method) & (two_clusterings.proxy_type == proxy)]\n",
    "        election_names = []\n",
    "        stdevs = [] \n",
    "        distances = []\n",
    "\n",
    "        for i, election in elections.iterrows():\n",
    "            clustering = election.clustering\n",
    "            centroids = election.proxies_of_centers\n",
    "            num_cands = election.num_cands\n",
    "            proxy_clustering = clustering_to_proxy(clustering, proxy, num_cands=num_cands)\n",
    "            weights_percent_0 = sum(clustering[0].values()) / (sum(clustering[0].values()) + sum(clustering[1].values()))\n",
    "            stdevs.append(cluster_stdev(centroids[0], proxy_clustering[0]))\n",
    "            stdevs.append(cluster_stdev(centroids[1], proxy_clustering[1]))\n",
    "            distances.append(weights_percent_0 * (np.sum(np.abs(np.array(centroids[0]) - np.array(centroids[1])))))\n",
    "            distances.append((1-weights_percent_0) * (np.sum(np.abs(np.array(centroids[0]) - np.array(centroids[1])))))\n",
    "\n",
    "        plt.scatter(distances, stdevs)\n",
    "        plt.title(f\"Standard Deviation vs Distance for {method} {proxy}\")\n",
    "        plt.xlabel(\"Distance between centroids (prorated by cluster weight))\")\n",
    "        plt.ylabel(\"Standard deviation\")\n",
    "        plt.savefig(f\"{PLOT_1_SAVE}/{method}_{proxy}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_1_SAVE = 'plots/stdev_v_stdev'\n",
    "for method in methods:\n",
    "    for proxy in proxies:\n",
    "        elections = two_clusterings[(two_clusterings.method == method) & (two_clusterings.proxy_type == proxy)]\n",
    "        election_names = []\n",
    "        stdev_1 = [] \n",
    "        stdev_2 = []\n",
    "\n",
    "        for i, election in elections.iterrows():\n",
    "            clustering = election.clustering\n",
    "            centroids = election.proxies_of_centers\n",
    "            num_cands = election.num_cands\n",
    "            proxy_clustering = clustering_to_proxy(clustering, proxy, num_cands=num_cands)\n",
    "            stdev_1.append(0.5*cluster_stdev(centroids[0], proxy_clustering[0]))\n",
    "            stdev_2.append(0.5*cluster_stdev(centroids[1], proxy_clustering[1]))\n",
    "\n",
    "        plt.scatter(stdev_1, stdev_2)\n",
    "        plt.title(f\"StDev vs StDev of each Cluster (unweighted) for {method} {proxy}\")\n",
    "        plt.xlabel(\"Standard Deviation (Cluster 1)\")\n",
    "        plt.ylabel(\"Standard Deviation (Cluster 2)\")\n",
    "        plt.savefig(f\"{PLOT_1_SAVE}/{method}_{proxy}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_1_SAVE = 'plots/stdev_v_stdev_weighted'\n",
    "for method in methods:\n",
    "    for proxy in proxies:\n",
    "        elections = two_clusterings[(two_clusterings.method == method) & (two_clusterings.proxy_type == proxy)]\n",
    "        election_names = []\n",
    "        stdev_1 = [] \n",
    "        stdev_2 = []\n",
    "\n",
    "        for i, election in elections.iterrows():\n",
    "            clustering = election.clustering\n",
    "            centroids = election.proxies_of_centers\n",
    "            num_cands = election.num_cands\n",
    "            proxy_clustering = clustering_to_proxy(clustering, proxy, num_cands=num_cands)\n",
    "            weights_percent_0 = sum(clustering[0].values()) / (sum(clustering[0].values()) + sum(clustering[1].values()))\n",
    "            stdev_1.append(weights_percent_0*cluster_stdev(centroids[0], proxy_clustering[0]))\n",
    "            stdev_2.append((1-weights_percent_0)*cluster_stdev(centroids[1], proxy_clustering[1]))\n",
    "\n",
    "        plt.scatter(stdev_1, stdev_2)\n",
    "        plt.title(f\"StDev vs StDev of each Cluster (weighted) for {method} {proxy}\")\n",
    "        plt.xlabel(\"Standard deviation weighted by cluster size (Cluster 1)\")\n",
    "        plt.ylabel(\"Standard deviation weighted by cluster size (Cluster 2)\")\n",
    "        plt.savefig(f\"{PLOT_1_SAVE}/{method}_{proxy}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def weighted_unique_coverage(centroids, points_list, alphas):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "      centroids   : list of centroid coordinates (each a list, tuple, or np.array)\n",
    "      points_list : list of dictionaries. Each dictionary maps a point (list, tuple, etc.)\n",
    "                    to its weight. The i-th dictionary corresponds to the i-th centroid.\n",
    "      alphas      : list of multipliers (one per centroid) for the standard deviation\n",
    "      \n",
    "    Returns:\n",
    "      (fraction_1x, fraction_alpha)\n",
    "      where:\n",
    "        - fraction_1x   is the weighted fraction of global points that lie within exactly\n",
    "                          one cluster’s 1× standard deviation threshold.\n",
    "        - fraction_alpha is the weighted fraction of global points that lie within exactly\n",
    "                          one cluster’s α× standard deviation threshold.\n",
    "    \"\"\"\n",
    "    n_clusters = len(centroids)\n",
    "    if not (len(points_list) == len(alphas) == n_clusters):\n",
    "        raise ValueError(\"Arrays centroids, points_list, and alphas must all be the same length.\")\n",
    "\n",
    "    # Step 1: Compute each cluster's weighted standard deviation and its thresholds.\n",
    "    thresholds = []       # 1× standard deviation thresholds for each cluster\n",
    "    alpha_thresholds = [] # α× standard deviation thresholds for each cluster\n",
    "    for centroid, pts_dict, alpha in zip(centroids, points_list, alphas):\n",
    "        c = np.array(centroid)\n",
    "        dists = []\n",
    "        ws = []\n",
    "        for pt, w in pts_dict.items():\n",
    "            pt_arr = np.array(pt)\n",
    "            # L1 distance from point to centroid\n",
    "            dist = np.sum(np.abs(pt_arr - c))\n",
    "            dists.append(dist)\n",
    "            ws.append(w)\n",
    "        dists = np.array(dists)\n",
    "        ws = np.array(ws)\n",
    "        total_w = np.sum(ws)\n",
    "        if total_w == 0:\n",
    "            raise ValueError(\"Total weight for a cluster cannot be zero.\")\n",
    "        # Compute weighted mean and variance (for the distances)\n",
    "        weighted_mean = np.sum(dists * ws) / total_w\n",
    "        variance = np.sum(ws * (dists - weighted_mean) ** 2) / total_w\n",
    "        stdev = np.sqrt(variance)\n",
    "        \n",
    "        thresholds.append(stdev)\n",
    "        alpha_thresholds.append(alpha * stdev)\n",
    "    \n",
    "    # Step 2: Combine all points from all clusters into a global dictionary.\n",
    "    # If the same point appears in multiple clusters, sum its weights.\n",
    "    global_points = {}\n",
    "    for pts_dict in points_list:\n",
    "        for pt, w in pts_dict.items():\n",
    "            # Ensure the point is hashable (e.g., convert lists to tuples)\n",
    "            pt_key = tuple(pt)\n",
    "            global_points[pt_key] = global_points.get(pt_key, 0) + w\n",
    "            \n",
    "    total_global_weight = sum(global_points.values())\n",
    "    if total_global_weight == 0:\n",
    "        raise ValueError(\"Global sum of weights cannot be zero.\")\n",
    "\n",
    "    # Step 3: For each global point, count in how many clusters it lies within the threshold.\n",
    "    numerator_1x = 0.0    # For points within exactly one cluster's 1× stdev threshold.\n",
    "    numerator_alpha = 0.0 # For points within exactly one cluster's α× stdev threshold.\n",
    "    \n",
    "    for pt, weight in global_points.items():\n",
    "        pt_arr = np.array(pt)\n",
    "        count_1x = 0\n",
    "        count_alpha = 0\n",
    "        # Check the point against every cluster.\n",
    "        for centroid, thresh, alphathresh in zip(centroids, thresholds, alpha_thresholds):\n",
    "            c = np.array(centroid)\n",
    "            dist = np.sum(np.abs(pt_arr - c))\n",
    "            if dist <= thresh:\n",
    "                count_1x += 1\n",
    "            if dist <= alphathresh:\n",
    "                count_alpha += 1\n",
    "        # Count the point only if it qualifies for exactly one cluster.\n",
    "        if count_1x == 1:\n",
    "            numerator_1x += weight\n",
    "        if count_alpha == 1:\n",
    "            numerator_alpha += weight\n",
    "    \n",
    "    fraction_1x = numerator_1x / total_global_weight\n",
    "    fraction_alpha = numerator_alpha / total_global_weight\n",
    "    \n",
    "    return fraction_1x, fraction_alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_SAVE = 'plots/stdev_coverage'\n",
    "plot_5_data = pd.DataFrame(columns=['filename', 'method', 'proxy_type', 'unweighted', 'weighted'])\n",
    "for method in methods:\n",
    "    for proxy in proxies:\n",
    "        elections = two_clusterings[(two_clusterings.method == method) & (two_clusterings.proxy_type == proxy)]\n",
    "        election_names = []\n",
    "        pct_covered = [] \n",
    "        weighted_pct_covered = []\n",
    "\n",
    "        for i, election in elections.iterrows():\n",
    "            clustering = election.clustering\n",
    "            centroids = election.proxies_of_centers\n",
    "            num_cands = election.num_cands\n",
    "            proxy_clustering = clustering_to_proxy(clustering, proxy, num_cands=num_cands)\n",
    "\n",
    "            weights_percent_0 = sum(clustering[0].values()) / (sum(clustering[0].values()) + sum(clustering[1].values()))\n",
    "            fraction, fraction_weighted = weighted_unique_coverage(centroids.values(), proxy_clustering.values(), [weights_percent_0, 1-weights_percent_0])\n",
    "            pct_covered.append(fraction)\n",
    "            weighted_pct_covered.append(fraction_weighted)\n",
    "            plot_5_data.loc[i] = {\n",
    "                'filename': election['filename'],\n",
    "                'method': method,\n",
    "                'proxy_type': proxy,\n",
    "                'unweighted': fraction,\n",
    "                'weighted': fraction_weighted\n",
    "            }\n",
    "\n",
    "        plt.scatter(pct_covered, weighted_pct_covered)\n",
    "        plt.title(f\"Fraction within one stdev of unique cluster ({method} {proxy})\", pad=20)\n",
    "        plt.title\n",
    "        plt.xlabel(\"Fraction within one stdev of unique cluster (unweighted)\")\n",
    "        plt.ylabel(\"Fraction within one stdev of unique cluster (weighted)\")\n",
    "        plt.savefig(f\"{PLOT_SAVE}/{method}_{proxy}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
