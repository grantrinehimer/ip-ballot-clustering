{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692b1271-6a51-4c03-9820-25f32f14ffde",
   "metadata": {},
   "source": [
    "%pip install gurobipy\n",
    "%pip install scikit-learn\n",
    "%pip install seaborn\n",
    "%pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a0779-48d8-481e-8980-7d42bdb05e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba5b01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import random\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b756babd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #import functions from Moon and Kris\n",
    "# from Clustering_Functions import parse, Summarize_election, Plot_ballot_lengths, Plot_clusters, \\\n",
    "#      kmeans, kmedoids, Random_clusters, Clustering_closeness, Cluster_mds_plot, Find_slates, Slate_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2696d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains functions for analyzing elections and clustering ballots. \n",
    "#This is copied from mggg to give me easy access to knowing what their functions are/do\n",
    "\n",
    "# An elections is represented as a dictionary matching ballots to weights.\n",
    "#   For example `{(1,3,4):5, (1): 7}` is the election where $5$ people cast the ballot $(1,3,4)$\n",
    "#   while $7$ people cast the bullet vote for candidate $1$.  \n",
    "#   The candidates are always named $1,2,...n$.\n",
    "\n",
    "# A clustering of an election means a partition its ballots.  \n",
    "#   Each cluster (each piece of the partition) is itself an election, \n",
    "#   so the clustering is represented as a list of elections.  \n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from itertools import permutations, combinations, chain\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances\n",
    "\n",
    "def remove_zeros(ballot):\n",
    "    \"\"\"\n",
    "    Helper function for parse.\n",
    "    \"\"\"\n",
    "    to_return = []\n",
    "    for vote in ballot:\n",
    "        if vote != 0:\n",
    "            to_return.append(vote)\n",
    "    return tuple(to_return)\n",
    "\n",
    "def parse(filename):\n",
    "    \"\"\"\n",
    "    Returns a tuple (election, names, location) obtained from parsing the format in which Scottish election data is stored.\n",
    "        \n",
    "    The returned election is a dictionary matching ballots to weights.  For example {(1,3,4):5, (1): 7} is the election where 5 people cast the ballot (1,3,4) while 7 people cast the bullet vote for candidate 1.\n",
    "    \n",
    "    The candidates are coded 1,2,...n in the ballots of the returned election.  The returned names tells the corresponding candidate names. \n",
    "\n",
    "    Args:\n",
    "        filename : name of file (.csv or .blt) containing the Scottish election data.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: election, names, location.\n",
    "    \"\"\"\n",
    "\n",
    "    election = {}\n",
    "    names = []\n",
    "    numbers = True\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file:\n",
    "            s = line.rstrip(\"\\n\").rstrip()\n",
    "            if numbers:\n",
    "                ballot = [int(vote) for vote in s.split(\" \")]\n",
    "                num_votes = ballot[0]\n",
    "                if num_votes == 0:\n",
    "                    numbers = False\n",
    "                else:\n",
    "                    election[remove_zeros(ballot[1:])] = num_votes\n",
    "            elif \"(\" not in s:\n",
    "                return election, names, s.strip(\"\\\"\")\n",
    "            else:\n",
    "                name_parts = s.strip(\"\\\"\").split(\" \")\n",
    "                first_name = \" \".join(name_parts[:-2])\n",
    "                last_name = name_parts[-2]\n",
    "                party = name_parts[-1].strip(\"(\").strip(\")\")\n",
    "                names.append((first_name, last_name, party))\n",
    "    raise Exception(f\"Error parsing file '{filename}'.\")\n",
    "\n",
    "def print_color(text,n): # print the text in the color associated to the integer index n.\n",
    "    \"\"\"\n",
    "    Helper function for Summarize_election\n",
    "    \"\"\"\n",
    "    black_code = \"\\033[00m\"\n",
    "    color_code = f\"\\033[{91+n}m\"\n",
    "    print(color_code,text,black_code)    \n",
    "\n",
    "def Summarize_election(election, clusters=None, size=10):\n",
    "    \"\"\"\n",
    "    Prints basic data about the given election including num candidates, num ballots, num distinct ballots, average ballot length, and the 10 (or any number) most often cast ballots.\n",
    "\n",
    "    If a clustering is also given, then it also prints this data saparately for each cluster and color codes by cluster the list of most commonly cast ballots.  \n",
    "\n",
    "    Args:\n",
    "        election : a dictionary matching ballots to weights (# times cast).\n",
    "        clusters : a clustering (list of elections) that partition the ballots of the election. \n",
    "        size : The desired length of the list of the most commonly cast ballots.\n",
    "    \"\"\"\n",
    "    all_ballots = [ballot for ballot in election.keys() if election[ballot]>0]\n",
    "    num_ballots = len(all_ballots)\n",
    "    candidates = sorted(list(set([item for ranking in all_ballots for item in ranking])))\n",
    "    num_cands = len(candidates)\n",
    "    num_voters = sum([election[ballot] for ballot in all_ballots])\n",
    "    mu = sum([len(ballot)*election[ballot] for ballot in election.keys()])/sum(election.values())\n",
    "    print(f\"This election has: {num_cands} candidates, {num_voters} ballots, {num_ballots} distinct ballots, {round(mu,2)} avg ballot length.\")\n",
    "\n",
    "    if not clusters==None:\n",
    "        for cluster_num in range(len(clusters)):\n",
    "            cluster = clusters[cluster_num]\n",
    "            all_ballots_c = [ballot for ballot in cluster.keys() if cluster[ballot]>0]\n",
    "            num_ballots_c = len(all_ballots_c)\n",
    "            num_voters_c = sum([cluster[ballot] for ballot in all_ballots_c])\n",
    "            mu_c = sum([len(ballot)*cluster[ballot] for ballot in cluster.keys()])/sum(cluster.values())           \n",
    "            print_color(f\"CLUSTER {cluster_num+1}: {num_voters_c} ballots, {num_ballots_c} distinct ballots, {round(mu_c,2)} avg ballot length.\",cluster_num)\n",
    "            \n",
    "    print(\"Top ballots:\")\n",
    "    ls = sorted(set(election.values()))\n",
    "    count = 0\n",
    "    broken = False\n",
    "    while not broken:\n",
    "        val = ls.pop()        \n",
    "        bs = [ballot for ballot in all_ballots if election[ballot]==val]\n",
    "        for ballot in bs:\n",
    "            if clusters == None:\n",
    "                color = -91 # black\n",
    "            else:\n",
    "                assignments = [n for n in range(len(clusters)) if ballot in clusters[n].keys() and clusters[n][ballot]>0]\n",
    "                if len(assignments)==1:\n",
    "                    color = assignments[0]\n",
    "                else:\n",
    "                    color = -84 # black outliner to indicate multiple clusters\n",
    "            print_color(f\"\\t {val} votes for {ballot}.\", color)\n",
    "            count +=1\n",
    "            if count>size:\n",
    "                broken = True\n",
    "                break\n",
    "\n",
    "def Plot_ballot_lengths(clusters, num_cands = 'Auto', filename=None):\n",
    "    \"\"\"\n",
    "    Plots a histogram of the ballot lengths for the given election or clustering.\n",
    "\n",
    "    If a clustering is given instead of an election, it superimposes histrograms for each cluster.  \n",
    "\n",
    "    Args:\n",
    "        clusters : either an election (a dictionary matching ballots to weights) or a clustering (a list of elections).\n",
    "        num_cands : the number of candidates.  Set to 'Auto' to ask the algorithm to determine it.\n",
    "        filename : to save the plot.\n",
    "    \"\"\"\n",
    "    if type(clusters)==dict:\n",
    "        clusters = [clusters]\n",
    "    k = len(clusters)\n",
    "    if num_cands == 'Auto':        \n",
    "        all_ballots = [x for cluster in clusters for x in cluster] \n",
    "        num_cands = max([item for ranking in all_ballots for item in ranking])\n",
    "\n",
    "    X = np.arange(num_cands)\n",
    "    width = .7/k\n",
    "    palat = ['grey','purple','tomato','orange','b','c','g', 'r', 'm', 'y', 'k']\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for clust in range(k):\n",
    "        Y = np.zeros(num_cands)\n",
    "        for ballot, weight in clusters[clust].items():\n",
    "            Y[len(ballot)-1]+=weight\n",
    "        Y = Y/sum(clusters[clust].values())\n",
    "        ax.bar(X+clust*width,Y, width=width, label = f\"Cluster {clust+1}\", color = palat[clust])\n",
    "    ax.set_title('Ballot lengths')\n",
    "    ax.set_xlabel('ballot length')\n",
    "    plt.xticks(X+(width*(k-1)/2) ,X+1)\n",
    "    plt.legend()\n",
    "    if filename == None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(filename)\n",
    "\n",
    "\n",
    "def Borda_vector(ballot_or_election,num_cands='Auto', borda_style='bord'):\n",
    "    # \"\"\"\n",
    "    # Returns the Borda vector of the given ballot or election.  The \"Borda vector of an election\" means the (weighted) sum of the Borda vectors of its ballots.\n",
    "\n",
    "    # Set borda_style = 'standard' for the standard convention that, in an election with n candidates, awards n-k points to the candidate in position k, and awards zero points for missing candidates.\n",
    "    \n",
    "    # Set borda_style = 'bord' for  to use the \"Conservative\" convenction that awards n-k-1 points to the candidate in position k, and awards zero points for missing candidates.\n",
    "    \n",
    "    # Set borda_style = 'full_points' for  to use the \"Averaged\" convenction that every ballot awards exactly 1+2+\\cdots+n Borda points; this is achieved for a short ballot by dividing the unawarded points equally among the missing candidates.\n",
    "\n",
    "        \n",
    "    # Args:\n",
    "    #     ballot_or_election : a single ballot (tuple) or an election (dictionary matching ballots to weights)\n",
    "    #     num_cands : the number of candidates.  Set to 'Auto' to ask the algorithm to determine it, but only if an election is given, since a single ballot isn't enough to determine num_cands.\n",
    "    #     borda_style : choice of {'standard', 'bord', 'full_points'}\n",
    "     \n",
    "    # Returns:\n",
    "    #     the Borda vector (np.array) of the given ballot or election.                \n",
    "    # \"\"\"\n",
    "    L = 1 if borda_style=='bord' else 0\n",
    "    # Borda vector of a ballot\n",
    "    if type(ballot_or_election) == tuple:\n",
    "        if num_cands=='Auto':\n",
    "            raise Exception(\"A single ballot is not enough to determine the number of candidates.\")\n",
    "        ballot = ballot_or_election\n",
    "        to_return = [0 for _ in range(num_cands)]\n",
    "        for count in range(len(ballot)):\n",
    "            candidate = ballot[count]\n",
    "            to_return[candidate-1] = num_cands-count - L\n",
    "        if borda_style=='full_points':\n",
    "            missing_cands = set(range(1,num_cands+1))-set(ballot)\n",
    "            for candidate in missing_cands:\n",
    "                to_return[candidate-1] += (len(missing_cands)+1)/2\n",
    "            \n",
    "    # Borda vector of an election\n",
    "    else:\n",
    "        election = ballot_or_election\n",
    "        if num_cands == 'Auto':\n",
    "            num_cands = max([item for ranking in election.keys() for item in ranking])\n",
    "\n",
    "        to_return = [0 for _ in range(num_cands)]\n",
    "        for ballot, ballot_weight in election.items():\n",
    "            for count in range(len(ballot)):\n",
    "                candidate = ballot[count]\n",
    "                to_return[candidate-1] += ballot_weight*(num_cands-count-L)\n",
    "            if borda_style=='full_points':\n",
    "                missing_cands = set(range(1,num_cands+1))-set(ballot)\n",
    "                for candidate in missing_cands:\n",
    "                    to_return[candidate-1] += ballot_weight*(len(missing_cands)+1)/2\n",
    "    \n",
    "    return np.array(to_return)\n",
    "\n",
    "def Borda_dist(CA, CB, num_cands = 'Auto', borda_style='bord', order = 1):\n",
    "    # \"\"\"\n",
    "    # Returns the L^p distance between the Borda vectors of the given pair of ballots or elections,\n",
    "    #     where p is called the order (for example, order=2 is the Euclidean distance).\n",
    "\n",
    "    # Set borda_style = 'standard' for the standard convention that, in an election with n candidates, awards n-k points to the candidate in position k, and awards zero points for missing candidates.\n",
    "    \n",
    "    # Set borda_style = 'bord' for  to use the \"Conservative\" convenction that awards n-k+1 points to the candidate in position k, and awards zero points for missing candidates.\n",
    "    \n",
    "    # Set borda_style = 'full_points' for  to use the \"Averaged\" convenction that every ballot awards exactly 1+2+\\cdots+n Borda points; this is achieved for a short ballot by dividing the unawarded points equally among the missing candidates.\n",
    "    \n",
    "    # Args:\n",
    "    #     CA, CB : a pair of ballots (tuples) or elections (dictionaries matching ballots to weights).\n",
    "    #     num_cands : the number of candidates.  Set to 'Auto' to ask the algorithm to determine it,\n",
    "    #                 but only if an election is given, since a ballot pair isn't enough to determine num_cands.\n",
    "    #     borda_style : choice of {'standard', 'bord', 'full_points'}\n",
    "    #     order : the choice of p with resepct to which the L^p distance is computed.\n",
    "    \n",
    "    # Returns:\n",
    "    #     the L^p distance between the Borda vectors.                \n",
    "    # \"\"\"\n",
    "    if num_cands == 'Auto':\n",
    "        if type(CA) == tuple:\n",
    "            raise Exception(\"A single pair of ballot is not enough to determine the number of candidates.\")\n",
    "        else:\n",
    "            all_ballots = list(CA.keys())+list(CB.keys())\n",
    "            num_cands = max([item for ranking in all_ballots for item in ranking])\n",
    "            \n",
    "    VA = Borda_vector(CA, num_cands=num_cands, borda_style=borda_style)\n",
    "    VB = Borda_vector(CB, num_cands=num_cands, borda_style=borda_style)\n",
    "    return np.linalg.norm(VA - VB,ord=order)\n",
    "\n",
    "def Candidate_matrix(election, num_cands = 'Auto'):\n",
    "    \"\"\"\n",
    "    Helper function for Plot_clusters   \n",
    "    \"\"\"\n",
    "    # Creates a matrix M such that M[i-1][c-1] is the number of ith place votes received by candidate c.\n",
    "    if num_cands == 'Auto':\n",
    "        num_cands = max([item for ranking in election.keys() for item in ranking])\n",
    "    \n",
    "    to_return = np.zeros([num_cands,num_cands])\n",
    "    for ballot in election.keys():\n",
    "        ballot_weight = election[ballot]\n",
    "        for ballot_position in range(len(ballot)):\n",
    "            candidate = ballot[ballot_position]\n",
    "            to_return[ballot_position][candidate-1] += ballot_weight\n",
    "    return to_return\n",
    "\n",
    "def Plot_clusters(clusters, method = 'Borda', borda_style='bord', num_cands = 'Auto', order = 'Auto', filename=None):\n",
    "    \"\"\"\n",
    "    Displays a bar plot that helps visualize the given election or clustering.\n",
    "\n",
    "    Args:\n",
    "        election: either an election (a dictionary matching ballots to weights) or a clustering (a list of elections).\n",
    "        method: either 'Borda' for a Borda plot, or 'Mentions' for a stacked mentions plot.\n",
    "        borda_style: choice of {'bord', 'standard', 'full_points'}, which is passed to Borda_vector.\n",
    "        num_cands : the number of candidates.  Set to 'Auto' to ask the algorithm to determine it.\n",
    "        order : Set order='Auto' to order the candidates by deceasing Borda scores in the first cluster.  Set say order=[3,2,4,1] to order the candidates according to the given list. \n",
    "        filename : to save the plot.     \n",
    "    \"\"\"\n",
    "    if type(clusters)==dict:\n",
    "        clusters = [clusters]\n",
    "    k = len(clusters)\n",
    "    if num_cands == 'Auto':        \n",
    "        all_ballots = [x for cluster in clusters for x in cluster] \n",
    "        num_cands = max([item for ranking in all_ballots for item in ranking])\n",
    "    if method=='Borda':\n",
    "        Scores = [Borda_vector(clusters[n], num_cands=num_cands, borda_style=borda_style) for n in range(k)]\n",
    "    else:\n",
    "        Scores = [Candidate_matrix(clusters[n], num_cands = num_cands) for n in range(k)]\n",
    "\n",
    "    if type(order)==list:\n",
    "        perm = [x-1 for x in order]\n",
    "    if order=='Auto': # Order candidates by Borda scores of first cluster\n",
    "        perm = np.flip(np.argsort(np.array(Borda_vector(clusters[0], num_cands=num_cands))))\n",
    "    if type(order)==list or order=='Auto':\n",
    "        Ordered_candidates = []\n",
    "        if method == 'Borda':\n",
    "            Ordered_scores = [np.zeros(num_cands) for _ in range(k)]\n",
    "        else:\n",
    "            Ordered_scores = [np.zeros([num_cands,num_cands]) for _ in range(k)]\n",
    "        for cand in range(num_cands):\n",
    "            Ordered_candidates.append(perm[cand]+1)\n",
    "            for clust in range(k):\n",
    "                if method == 'Borda':\n",
    "                    Ordered_scores[clust][cand] = Scores[clust][perm[cand]]\n",
    "                else:\n",
    "                    for ballot_position in range(num_cands):\n",
    "                        Ordered_scores[clust][ballot_position,cand] = Scores[clust][ballot_position,perm[cand]]\n",
    "    else:\n",
    "        Ordered_candidates = list(range(1,num_cands+1))\n",
    "        Ordered_scores = Scores\n",
    "\n",
    "    palat = ['grey','purple','tomato','orange','b','c','g', 'r', 'm', 'y', 'k']\n",
    "    r = np.arange(num_cands)\n",
    "    width = 0.7/k\n",
    "    bottoms = [np.zeros(num_cands) for _ in range(k)]\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for clust in range(k):\n",
    "        if method == 'Borda':\n",
    "            pA = ax.bar(r + clust*width, Ordered_scores[clust], label=f\"Cluster {clust+1}\", color = palat[clust],\n",
    "                width = width, edgecolor = 'black')\n",
    "        else:\n",
    "            for shade in range(1,num_cands+1):\n",
    "                Shade_scores = Ordered_scores[clust][shade-1]\n",
    "                label = f\"Cluster {clust+1}\" if shade==1 else None\n",
    "                pA = ax.bar(r+clust*width, Shade_scores, width=width, bottom = bottoms[clust], \n",
    "                            color = (palat[clust],1/shade), label=label)\n",
    "                bottoms[clust] += Shade_scores\n",
    "\n",
    "    if method == 'Borda':\n",
    "        ax.set_title('Borda Scores of Candidates by Cluster')\n",
    "    else:\n",
    "        ax.set_title('Candidate Mentions Stacked by Ballot Position')\n",
    "    ax.set_xlabel('Candidate')\n",
    "    plt.xticks(r + (width*(k-1))/2,Ordered_candidates)\n",
    "    plt.legend()\n",
    "    if filename == None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(filename)\n",
    "\n",
    "def HH_proxy(ballot,num_cands):\n",
    "    \"\"\"\n",
    "    Returns the head-to-head proxy vector of the given ballot.\n",
    "        \n",
    "    This is a vector with one entry for each pair of candidates ordered in the natural way; namely {(1,2),(1,3),...,(1,n),(2,3),...}.  The entries lie in {-1/2,0,1/2} depending on whether the lower-indexed candidate {looses, ties, wins} the head-to-head comparison. \n",
    "\n",
    "    Args:\n",
    "        ballot: a single ballot (tuple)\n",
    "    \n",
    "    Returns:\n",
    "        The head-to-head proxy vector (np.array)\n",
    "    \"\"\"\n",
    "    M = np.zeros([num_cands,num_cands])\n",
    "    for x,y in combinations(ballot,2):\n",
    "        M[x-1,y-1] = 1/2\n",
    "        M[y-1,x-1] = -1/2\n",
    "    for x in ballot:\n",
    "        for y in set(range(1,num_cands+1)) - set(ballot): # candidates missing from the ballot\n",
    "            M[x-1,y-1] = 1/2\n",
    "            M[y-1,x-1] = -1/2\n",
    "    to_return = []\n",
    "    for x,y in combinations(range(num_cands),2):\n",
    "        to_return.append(M[x,y])\n",
    "    return np.array(to_return)\n",
    "\n",
    "def HH_dist(ballot1, ballot2, num_cands, order = 1):\n",
    "    \"\"\"\n",
    "    Returns the L^p distance between the head-to-head proxy vectors of the given pair of ballots, where p is called the order.\n",
    "\n",
    "    Args:\n",
    "        ballot1 : ballot (tuple)\n",
    "        ballot2 : ballot (tuple)\n",
    "        num_cands : the number of candidates\n",
    "        order : the choice of p with respect to which the L^p distance is computed\n",
    "    \n",
    "    Returns:\n",
    "        The L^p distance between the proxy vectors.   \n",
    "    \"\"\"\n",
    "    H1 = HH_proxy(ballot1, num_cands=num_cands)\n",
    "    H2 = HH_proxy(ballot2, num_cands=num_cands)\n",
    "    return np.linalg.norm(H1-H2,ord=order)\n",
    "\n",
    "def kmeans(election, k=2, proxy='Borda', borda_style='bord', n_init=200, return_centroids=False):\n",
    "    \"\"\"\n",
    "    Returns the clustering obtained by applying the k-means algorithm to the proxies of the ballots.\n",
    "\n",
    "    Args:\n",
    "        election : dictionary matching ballots with weights.\n",
    "        k : the number of clusters desired.\n",
    "        proxy : choice of {'Borda', 'HH'} for Borda or head-to-head proxy vectors.\n",
    "        borda_style : choice of {'bord', 'standard', 'full_points'}, which is passed to Borda_vector (only if proxy == 'Borda') \n",
    "        n_init : the algorithm runs n_init independent times with different starting centers each time, and outputs the clustering that has the best score from all the runs.\n",
    "        return_centroids : set to True if you want it to also return the centroids of the returned clustering.\n",
    "\n",
    "    Returns:\n",
    "        if return_centroids == False: returns a clustering (list of elections).\n",
    "        if return_centroids == True: returns a tuple (clustering, centroids).\n",
    "    \"\"\"\n",
    "    all_ballots = list(election.keys())\n",
    "    num_ballots = len(all_ballots)\n",
    "    candidates = sorted(list(set([item for ranking in all_ballots for item in ranking])))\n",
    "    num_cands = len(candidates)\n",
    "    sample_weight = np.array([election[ballot] for ballot in all_ballots])\n",
    "    if proxy=='Borda':\n",
    "        X = np.array([Borda_vector(ballot, num_cands=num_cands, borda_style=borda_style) \n",
    "                      for ballot in all_ballots])\n",
    "    else:\n",
    "        X = np.array([HH_proxy(ballot,num_cands=num_cands) for ballot in all_ballots])\n",
    "    \n",
    "    model = KMeans(n_clusters=k, n_init=n_init).fit(X,sample_weight=sample_weight)\n",
    "    labels = model.labels_\n",
    "    centroids = model.cluster_centers_\n",
    "    \n",
    "    C = [dict() for _ in range(k)]\n",
    "    for count in range(len(all_ballots)):\n",
    "        ballot = all_ballots[count]\n",
    "        C[labels[count]][ballot]=election[ballot]\n",
    "    if return_centroids:\n",
    "        return C, centroids\n",
    "    else:\n",
    "        return C\n",
    "    \n",
    "def Manhattan_dist(A,B):\n",
    "    return sum(np.abs(A-B))\n",
    "\n",
    "def kmedoids(election, k=2, proxy='Borda', borda_style='bord', verbose = False,\n",
    "             method = 'pam', share_ties = True, return_medoids=False):\n",
    "    \"\"\"\n",
    "    Returns the clustering obtained by applying the k-medoid algorithm to the proxies of the ballots.\n",
    "\n",
    "    Args:\n",
    "        election : dictionary matching ballots with weights.\n",
    "        k : the number of clusters desired.\n",
    "        proxy : choice of {'Borda', 'HH'} for Borda or head-to-head proxy vectors.\n",
    "        borda_style : choice of {'bord', 'standard', 'full_points'}, which is passed to Borda_vector (only if proxy == 'Borda') \n",
    "        verbose : set to True if you want it to print the medoids.\n",
    "        method : choice of {'pam','alternate'}.  The method 'pam' is more accurate, while 'alternate' is faster\n",
    "        share_ties : set to True if you want the weight of any ballot that's equidistant to mulitple medoids to be shared between the corresponding clusters in the final iteration. This requires overlaid code because sklearn gives ties to the lowest-indexed cluster (which causes repeatability isses).  \n",
    "        return_medoids : set to True if you want it to also return the medoids of the returned clustering.\n",
    "\n",
    "    Returns:\n",
    "        if return_medoids == False: returns a clustering (list of elections).\n",
    "        if return_medoids == True: returns a tuple (clustering, medoids).\n",
    "    \"\"\"\n",
    "    num_cands = max([item for ranking in election.keys() for item in ranking])\n",
    "\n",
    "    # create a matrix whose rows are the ballots (repeated as many times as the ballot was cast) \n",
    "    # and a dictionary matching each ballot type with its first corresponding row in the matrix\n",
    "    # and a reverse dictionary to match each row number of the matrix with a ballot\n",
    "    X = []\n",
    "    ballot_to_row = dict()\n",
    "    row_to_ballot = dict()\n",
    "    counter = 0\n",
    "    for ballot, weight in election.items():\n",
    "        ballot_to_row[ballot]=counter\n",
    "        for _ in range(weight):\n",
    "            if proxy=='Borda':\n",
    "                X.append(Borda_vector(ballot, num_cands=num_cands, borda_style=borda_style))\n",
    "            else:\n",
    "                X.append(HH_proxy(ballot,num_cands=num_cands))\n",
    "            row_to_ballot[counter]=ballot\n",
    "            counter +=1\n",
    "    \n",
    "    model = KMedoids(n_clusters=k, metric=\"manhattan\", method = method, init = 'k-medoids++').fit(X)\n",
    "    labels = model.labels_\n",
    "    medoids = model.cluster_centers_\n",
    "    medoid_ballots = [row_to_ballot[index] for index in model.medoid_indices_]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Medoids = {medoid_ballots}.\")\n",
    "\n",
    "    # convert labels into a clustering (list of dictionaries)\n",
    "    C = [dict() for _ in range(k)]\n",
    "    if share_ties:\n",
    "        total_shared_weight = 0\n",
    "        for ballot, weight in election.items():\n",
    "            proxy = X[ballot_to_row[ballot]]\n",
    "            dists = [Manhattan_dist(medoid,proxy) for medoid in medoids]\n",
    "            clusts = [x for x in range(k) if dists[x]==np.min(dists)] # multi-valued argmin\n",
    "            if len(clusts)>1:\n",
    "                total_shared_weight +=weight\n",
    "            for clust in clusts:\n",
    "                C[clust][ballot]=weight/len(clusts)\n",
    "        #if verbose:\n",
    "        #    print(f\"Portion of ballots that tied = {total_shared_weight/sum(election.values())}\")\n",
    "\n",
    "    else:\n",
    "        for ballot, weight in election.items():\n",
    "            lab = labels[ballot_to_row[ballot]]\n",
    "            C[lab][ballot]=weight\n",
    "\n",
    "    if return_medoids:\n",
    "        return C, medoid_ballots\n",
    "    else:\n",
    "        return C\n",
    "    \n",
    "def Random_clusters(election,k=2): # returns a random clustering of the ballots.\n",
    "    \"\"\" \n",
    "    Returns the clustering obtained by performing a random partition of the ballots.\n",
    "    The full weight if each ballot is put into a single randomly selected one of the clusters.\n",
    "\n",
    "    Args:\n",
    "        election : dictionary matching ballots with weights.\n",
    "        k : the number of clusters desired.\n",
    "    \n",
    "    Returns:\n",
    "        a clustering (list of elections).\n",
    "    \"\"\"\n",
    "    C = [dict() for _ in range(k)]\n",
    "    for ballot in list(election.keys()):\n",
    "        die = random.randint(0,k-1)\n",
    "        C[die][ballot] = election[ballot]\n",
    "    return C\n",
    "\n",
    "def Clustering_closeness(election,C1,C2, num_cands = 'Auto'):\n",
    "    \"\"\"\n",
    "    Returns the closeness of the given two clusterings, which means the portion of the total ballots for which the two partitions differ (with respect to the best matching of one partition's two clusters with the other's two clusters)\n",
    "    \n",
    "    Args:\n",
    "        election : a dictionary matching ballots to weights.\n",
    "        C1 : a clustering (list of elections) which must have exactly 2 clusters.\n",
    "        C2 : a clustering (list of elections) which must have exactly 2 clusters.\n",
    "        num_cands : the number of candidates.  Set to 'Auto' to ask the algorithm to determine it.\n",
    "\n",
    "    Returns:\n",
    "        The closeness of the two clusterings, which equals 0 of they are identical and equals about .5 if they are as unrelated as would be a random pair of clusterings.   \n",
    "    \"\"\"\n",
    "    if num_cands == 'Auto':\n",
    "        num_cands = max([item for ranking in election.keys() for item in ranking])\n",
    "    matchAA = 0\n",
    "    matchAB = 0\n",
    "    for ballot in election.keys():\n",
    "        W1A = C1[0][ballot] if ballot in C1[0].keys() else 0\n",
    "        W1B = C1[1][ballot] if ballot in C1[1].keys() else 0\n",
    "        W2A = C2[0][ballot] if ballot in C2[0].keys() else 0\n",
    "        W2B = C2[1][ballot] if ballot in C2[1].keys() else 0\n",
    "        matchAA += np.abs(W1A-W2A) \n",
    "        matchAB += np.abs(W1A-W2B)\n",
    "    return min(matchAA,matchAB)/sum(election.values())\n",
    "\n",
    "def Cluster_mds_plot(election, clusters, proxy='Borda', borda_style='bord', threshold=10, label_threshold = np.infty,\n",
    "                     metric = 'Euclidean', plot_centers = True, filename=None):\n",
    "    \"\"\"\n",
    "    Displays an MDS (multi-dimensional scaling) plot for the proxies of all of the ballots in the election that received at least the given threshold number of votes, colored by their cluster assignments.\n",
    "\n",
    "    Args:\n",
    "        election : a dictionary matching ballots to weights.\n",
    "        clusters : a clustering (list of elections that partitions the given election.)\n",
    "        proxy : choice of {'Borda', 'HH'} for Borda or head-to-head proxy vectors.\n",
    "        borda_style : choice of {'bord', 'standard', 'full_points'}, which is passed to Borda_vector (only used if proxy == 'Borda') \n",
    "        threshold : it ignores all ballots that were cast fewer than the threshold number of times.\n",
    "        label_threshold : it labels all ballots that were cast at least the label_threshold number of times (set label_threshold=np.infty for no labeling)\n",
    "        metric : choice of {'Euclidean', 'Manhattan'} for the proxy metric that's approximated.\n",
    "        plot_centers : set True to plot the centroid of each cluster as a \"+\" symbol and to display the medoid of each cluster as a thickened circle.\n",
    "        filename : to save the plot.   \n",
    "    \"\"\"\n",
    "\n",
    "    num_cands = max([item for ranking in election.keys() for item in ranking])\n",
    "\n",
    "    ballots = []\n",
    "    proxies = []\n",
    "    weights = []\n",
    "    cluster_assignments = []\n",
    "    is_medoid = []\n",
    "    is_centroid = []\n",
    "\n",
    "    for cluster_num in range(len(clusters)):\n",
    "        cluster = clusters[cluster_num]\n",
    "        start_index = len(proxies)\n",
    "        these_proxies = []\n",
    "        these_weights = []\n",
    "        for ballot,weight in cluster.items():\n",
    "            if weight>=threshold:\n",
    "                if proxy=='Borda':\n",
    "                    ballot_proxy = Borda_vector(ballot,num_cands=num_cands, borda_style=borda_style)\n",
    "                else:\n",
    "                    ballot_proxy = HH_proxy(ballot,num_cands=num_cands)\n",
    "                ballots.append(ballot)\n",
    "                proxies.append(ballot_proxy)\n",
    "                these_proxies.append(ballot_proxy)\n",
    "                is_centroid.append(False)\n",
    "                is_medoid.append(False)\n",
    "                weights.append(weight)\n",
    "                these_weights.append(weight)\n",
    "                cluster_assignments.append(cluster_num)\n",
    "        if plot_centers:\n",
    "            if metric == 'Euclidean':\n",
    "                these_similarities = euclidean_distances(these_proxies)\n",
    "            else:\n",
    "                these_similarities = manhattan_distances(these_proxies)\n",
    "            these_weights = np.array(these_weights)\n",
    "            these_proxies = np.array(these_proxies)\n",
    "            row_sums = [np.dot(row,these_weights) for row in these_similarities]\n",
    "            medoid_index = np.argmin(row_sums)\n",
    "            is_medoid[start_index+medoid_index] = True\n",
    "            centroid = [np.dot(these_proxies[:,col_num],these_weights) for col_num in range(len(these_proxies[0]))]\n",
    "            centroid = np.array(centroid)/sum(these_weights)\n",
    "            proxies.append(centroid)\n",
    "            ballots.append(())\n",
    "            weights.append(1)\n",
    "            cluster_assignments.append(cluster_num)\n",
    "            is_medoid.append(False)\n",
    "            is_centroid.append(True)\n",
    "            \n",
    "    if metric == 'Euclidean':\n",
    "        similarities = euclidean_distances(proxies)\n",
    "    else:\n",
    "        similarities = manhattan_distances(proxies)\n",
    "\n",
    "    projections = MDS(n_components=2, dissimilarity='precomputed').fit_transform(similarities)\n",
    "    X = np.array([p[0] for p in projections])\n",
    "    Y = np.array([p[1] for p in projections])\n",
    "\n",
    "    palat = ['grey','purple','tomato','orange','b','c','g', 'r', 'm', 'y']\n",
    "    colors = [palat[x] for x in cluster_assignments]\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X,Y, s = weights, c = colors, alpha = .5)\n",
    "    ax.set_title('MDS Plot')\n",
    "    ax.grid(False)\n",
    "    ax.axis('off')\n",
    "    for count in range(len(proxies)):\n",
    "        if weights[count]>label_threshold:\n",
    "            ax.annotate(ballots[count], xy=(X[count], Y[count]))\n",
    "    if plot_centers:\n",
    "        # Plot medoids\n",
    "        X_0 = [X[t] for t in range(len(X)) if is_medoid[t]]\n",
    "        Y_0 = [Y[t] for t in range(len(Y)) if is_medoid[t]]\n",
    "        colors_0 = [colors[t] for t in range(len(colors)) if is_medoid[t]]\n",
    "        weights_0 = [weights[t] for t in range(len(weights)) if is_medoid[t]]\n",
    "        ax.scatter(X_0,Y_0, s = weights_0, c = colors_0, alpha = .5, edgecolors='k', linewidth=4)\n",
    "\n",
    "        # Plot centroids\n",
    "        X_0 = [X[t] for t in range(len(X)) if is_centroid[t]]\n",
    "        Y_0 = [Y[t] for t in range(len(Y)) if is_centroid[t]]\n",
    "        ax.scatter(X_0,Y_0, c = 'k', marker='P')\n",
    "\n",
    "    if filename == None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(filename)\n",
    "\n",
    "## Helper functions for Find_slates  \n",
    "def Matrix_merge(M,i,j): # Merges index i and j of the given matrix.  Assumes i<j.\n",
    "    \"\"\"\n",
    "    Helper function for Find_slates.   \n",
    "    \"\"\"\n",
    "    n = len(M)\n",
    "    to_return = np.zeros([n-1,n-1])\n",
    "    for x in range(n-1):\n",
    "        for y in range(n-1):\n",
    "            xp = x if x<j else x+1\n",
    "            yp = y if y<j else y+1\n",
    "            if (x==i and y==i):\n",
    "                value = 0\n",
    "            elif x==i:\n",
    "                value = M[i,yp]+M[j,yp]\n",
    "            elif y==i:\n",
    "                value = M[xp,i]+M[xp,j]\n",
    "            else:\n",
    "                value = M[xp,yp]\n",
    "            to_return[x,y] = value\n",
    "    return to_return\n",
    "\n",
    "def List_merge(L,i,j): # Merges entries i and j of the given list.\n",
    "    \"\"\"\n",
    "    Helper function for Find_slates.   \n",
    "    \"\"\"\n",
    "    n = len(L)\n",
    "    to_return = []\n",
    "    for x in range(n-1):\n",
    "        offset = 0 if x<j else 1\n",
    "        if x<i:\n",
    "            to_return.append(L[x])\n",
    "        elif x==i:\n",
    "            to_return.append(L[i].union(L[j]))\n",
    "        else:\n",
    "            to_return.append(L[x+offset])\n",
    "    return to_return\n",
    "\n",
    "def Candidate_weight_dict(L): \n",
    "    \"\"\"\n",
    "    Helper function for Find_slates.   \n",
    "    \"\"\"\n",
    "    # returns a dictionary matching candidate numbers to positions in the partial order.\n",
    "    DR = dict()\n",
    "    for count in range(len(L)):\n",
    "        for candidate in L[count]:\n",
    "            DR[candidate] = count\n",
    "    return DR\n",
    "\n",
    "def Success_count(election, L, num_cands = 'Auto'): \n",
    "    \"\"\"\n",
    "    Helper function for Find_slates.   \n",
    "    \"\"\"\n",
    "    # returns the portion of ballots in the election that are weakly consistent with L.\n",
    "\n",
    "    if num_cands == 'Auto':\n",
    "        num_cands = max([item for ranking in election.keys() for item in ranking])\n",
    "    DL = Candidate_weight_dict(L)\n",
    "    Yes_count = 0\n",
    "    No_count = 0\n",
    "    for ballot, weight in election.items():\n",
    "        broke = False\n",
    "        for a,b in combinations(ballot,2):\n",
    "            if DL[a]>DL[b]:\n",
    "                broke = True\n",
    "                break\n",
    "        for a in ballot:\n",
    "            for b in set(range(1,num_cands+1)) - set(ballot): # candidates missing from the ballot\n",
    "                if DL[a]>DL[b]:\n",
    "                    broke = True\n",
    "                    break \n",
    "        if broke:\n",
    "            No_count +=weight\n",
    "        else:\n",
    "            Yes_count +=weight\n",
    "    return Yes_count/(Yes_count+No_count)\n",
    "\n",
    "def Convert_sums_to_averages(M,L):\n",
    "    \"\"\"\n",
    "    Helper function for Find_slates.   \n",
    "    \"\"\"\n",
    "    n = len(M)\n",
    "    M_avg = np.zeros([n,n])\n",
    "    for x in range(n):\n",
    "        for y in range(n):\n",
    "            M_avg[x,y] = M[x,y]/(len(L[x])*len(L[y]))\n",
    "    return(M_avg)\n",
    "\n",
    "def reorder(M,L): # returns re-ordering of L by popularity.\n",
    "    \"\"\"\n",
    "    Helper function for Find_slates.   \n",
    "    \"\"\"\n",
    "    n = len(M)\n",
    "    M_avg = Convert_sums_to_averages(M,L)\n",
    "    M_sums = [sum(M_avg[i]) for i in range(len(M))]\n",
    "    perm = np.flip(np.argsort(M_sums))\n",
    "    L_ordered=[]\n",
    "    for count in range(len(L)):\n",
    "        L_ordered.append(L[perm[count]])\n",
    "    return L_ordered\n",
    "\n",
    "def Initial_matrix(election,num_cands):\n",
    "    \"\"\"\n",
    "    Helper function for Find_slates.   \n",
    "    \"\"\"\n",
    "    to_return = np.zeros([num_cands,num_cands])\n",
    "    for ballot, ballot_weight in election.items():\n",
    "        for x,y in combinations(ballot,2):\n",
    "            to_return[x-1,y-1] += ballot_weight\n",
    "        for x in ballot:\n",
    "            for y in set(range(1,num_cands+1)) - set(ballot): # candidates missing from the ballot\n",
    "                to_return[x-1,y-1] += ballot_weight\n",
    "    return to_return\n",
    "\n",
    "def Find_slates(clusters, num_cands='Auto', num_steps='Auto'):\n",
    "    \"\"\"\n",
    "    Prints the result of our algorithm that find a slate (bipartition of the candidates) matching he given clustering.  The algorithm iteratively merges candidates together to form slates and displays each step of this iterative merging.  \n",
    "\n",
    "    Args:\n",
    "        clusters : either an election (dictionary matching ballots to weights) or a clustering (list of elections)\n",
    "        num_cands : The number of candidates.  Set to 'Auto' to ask the algorithm to determine it.\n",
    "        num_steps : should at most be one less than the number of candidates (which is the default you get by setting num_steps = 'Auto'). \n",
    "    \"\"\"\n",
    "    if type(clusters)==dict:\n",
    "        clusters = [clusters]\n",
    "    k = len(clusters)\n",
    "    if num_cands == 'Auto':        \n",
    "        all_ballots = [x for cluster in clusters for x in cluster] \n",
    "        num_cands = max([item for ranking in all_ballots for item in ranking])\n",
    "    if num_steps == 'Auto':\n",
    "        num_steps = num_cands-1\n",
    "\n",
    "    L = [{n} for n in range(1,num_cands+1)]  # initial list of candidate-sets\n",
    "    M = [Initial_matrix(clusters[c], num_cands = num_cands) for c in range(k)] # initial HH-matrix\n",
    "    M_avg = [Initial_matrix(clusters[c], num_cands = num_cands) for c in range(k)] # avg HH-matrix\n",
    "    # The index i in M corresponds to candidate-set L[i]\n",
    "    \n",
    "    # Repeatedly collapse the list and the matrices\n",
    "    for count in range(num_steps+1):\n",
    "        if count>0:\n",
    "            n = len(L)\n",
    "            best = [0,0,0]\n",
    "            for i,j in permutations(range(n),2):\n",
    "                backflows = [min(M_avg[c][i,j],M_avg[c][j,i]) for c in range(k)]\n",
    "                next = (i,j,max(backflows))\n",
    "                if next[2]>best[2]:\n",
    "                    best = next        \n",
    "            if best[2]>0:\n",
    "                L = List_merge(L,best[0],best[1])\n",
    "                #print(best[0],best[1],L)\n",
    "                for c in range(k):\n",
    "                    M[c] = Matrix_merge(M[c],best[0],best[1])\n",
    "                    M_avg[c] = Convert_sums_to_averages(M[c],L)\n",
    "        \n",
    "        # print report\n",
    "        for c in range(k):\n",
    "            LR = reorder(M[c],L)\n",
    "            LR_pretty = str(LR).replace(\"},\",\"} >=\").replace(\"[\",\" \").replace(\"]\",\"\")\n",
    "            success = round(Success_count(clusters[c], LR, num_cands = num_cands),2)\n",
    "            print(f\"{success} of cluster {c} ballots: {LR_pretty}.\")\n",
    "        print('---')\n",
    "\n",
    "def powerset(iterable): # returns a list of the nontrival non-full subsets of the given iterable\n",
    "    \"\"\"\n",
    "    Helper function for Slate_cluster   \n",
    "    \"\"\"\n",
    "    s = list(iterable)\n",
    "    l = list(chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))\n",
    "    l.pop()    # remove the full set from the end of the list\n",
    "    l.pop(0)   # remove the empty set from the start of the list\n",
    "    return l\n",
    "\n",
    "# The following returns the pair of HH Proxies for two partial orders\n",
    "# associated to the slate A\n",
    "def HH_vectors_of_slate(A,num_cands):\n",
    "    \"\"\"\n",
    "    Helper function for Slate_cluster    \n",
    "    \"\"\"\n",
    "    B = tuple(set(range(1,num_cands+1))-set(A)) # the compliment of A\n",
    "    M = np.zeros([num_cands,num_cands])\n",
    "    for x in A:\n",
    "        for y in B:\n",
    "            M[x-1,y-1] = 1/2\n",
    "            M[y-1,x-1] = -1/2\n",
    "    to_return = []\n",
    "    for x,y in combinations(range(num_cands),2):\n",
    "        to_return.append(M[x,y])\n",
    "    return np.array(to_return), (-1)*np.array(to_return)\n",
    "\n",
    "def Slate_cluster(election, verbose = True, Delta = True, share_ties = True):\n",
    "    \"\"\"\n",
    "    Returns a clustering with k=2 clusters using a slate-based method based the distance that ballots are from being strongly consistent.\n",
    "    \n",
    "    For each slate S={A,B} (each bi-partition of the candidates), the slate's score is computed as the sum (over the weighted ballots in the election) of the ballot's distance to the closest condition: $A>B$ or $B>A$.\n",
    "    \n",
    "    Note that a ballot has zero distance iff it is strongly consistent.  The slate with the minimal score is used to partition the ballots into 2 clusters.\n",
    "\n",
    "    If verbose == True, the slate is printed coded as a tuple that represents the first half of a bipartition of the candidates. For example the slate (1,3,5) codes for the partition {1,3,5},{2,4,6} (with 6 candidates).\n",
    "\n",
    "    Args:\n",
    "        election : dictionary matching ballots to weights.\n",
    "        verbose : boolean. \n",
    "        Delta :  set Delta=False to use the simpler (Delta-free) measurement that says the distance from a ballot and a condition is just the distance between their proxies.\n",
    "    \n",
    "    Returns:\n",
    "        A clustering (list of elections).\n",
    "    \"\"\"\n",
    "    num_cands = max([item for ranking in election.keys() for item in ranking])\n",
    "    # create a matrix X whose rows are the HH proxies of the unique ballots\n",
    "    # and a dictionary matching each ballot type with its corresponding row in the matrix\n",
    "    # and a reverse dictionary to match each row number of the matrix with a ballot\n",
    "    X = []\n",
    "    ballot_to_row = dict()\n",
    "    row_to_ballot = dict()\n",
    "    counter = 0\n",
    "    num_ballots = 0\n",
    "    for ballot, weight in election.items():\n",
    "        num_ballots += weight\n",
    "        ballot_to_row[ballot]=counter\n",
    "        row_to_ballot[counter]=ballot\n",
    "        X.append(HH_proxy(ballot,num_cands=num_cands))\n",
    "        counter +=1\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_subset = tuple()\n",
    "    \n",
    "    # Determine the best slate\n",
    "    for A in powerset(range(1,num_cands+1)):\n",
    "        B = tuple(set(range(1,num_cands+1))-set(A)) # the compliment of A\n",
    "        A_slate_size = len(A)\n",
    "        B_slate_size = len(B)\n",
    "        A_proxy, B_proxy = HH_vectors_of_slate(A,num_cands)\n",
    "        slate_score = 0\n",
    "        \n",
    "        for ballot, weight in election.items(): # compute dist from the ballot to the slate\n",
    "            ballot_proxy = X[ballot_to_row[ballot]]\n",
    "            A_size = len(set(A).intersection(set(ballot)))\n",
    "            B_size = len(set(B).intersection(set(ballot)))\n",
    "            diag_points =(math.comb(A_size,2) - math.comb(A_slate_size-A_size,2) \\\n",
    "                        + math.comb(B_size,2) - math.comb(B_slate_size-B_size,2))/2\n",
    "            A_dist = np.linalg.norm(A_proxy-ballot_proxy,ord=1) - diag_points\n",
    "            B_dist = np.linalg.norm(B_proxy-ballot_proxy,ord=1) - diag_points\n",
    "            dist = min(A_dist,B_dist)\n",
    "            slate_score += dist*weight\n",
    "        if slate_score<best_score:\n",
    "            best_score = slate_score\n",
    "            best_subset = A\n",
    "    if verbose:\n",
    "        print(f\"Slate = {best_subset}.\")\n",
    "\n",
    "    # Form clusters from the best slate\n",
    "    A = best_subset\n",
    "    B = tuple(set(range(1,num_cands+1))-set(A)) # the compliment of A\n",
    "    A_slate_size = len(A)\n",
    "    B_slate_size = len(B)\n",
    "    A_proxy, B_proxy = HH_vectors_of_slate(A,num_cands)\n",
    "    CA = dict()\n",
    "    CB = dict()\n",
    "    total_shared_weight = 0\n",
    "    \n",
    "    for ballot, weight in election.items():\n",
    "        ballot_proxy = X[ballot_to_row[ballot]]\n",
    "        A_size = len(set(A).intersection(set(ballot)))\n",
    "        B_size = len(set(B).intersection(set(ballot)))\n",
    "        if Delta:\n",
    "            diag_points =(math.comb(A_slate_size,2) - math.comb(A_slate_size-A_size,2) \\\n",
    "                        + math.comb(B_slate_size,2) - math.comb(B_slate_size-B_size,2))/2\n",
    "        else:\n",
    "            diag_points = 0\n",
    "        A_dist = np.linalg.norm(A_proxy-ballot_proxy,ord=1) - diag_points\n",
    "        B_dist = np.linalg.norm(B_proxy-ballot_proxy,ord=1) - diag_points\n",
    "        if share_ties and A_dist == B_dist:\n",
    "            CA[ballot]=weight/2\n",
    "            CB[ballot]=weight/2\n",
    "            total_shared_weight +=weight\n",
    "        elif A_dist<B_dist:\n",
    "            CA[ballot]=weight\n",
    "        else:\n",
    "            CB[ballot]=weight\n",
    "    # if verbose:\n",
    "    #    print(f\"Portion of ballots that tied = {total_shared_weight/sum(election.values())}\")\n",
    "    \n",
    "    return CA,CB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ec73087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up for HH metric (one sample data set)\n",
    "election, cand_names, location = parse('data/edinburgh17-02.blt')\n",
    "#num_cands = max([item for ranking in election.keys() for item in ranking])\n",
    "\n",
    "all_ballots = list(election.keys())\n",
    "num_ballots = len(all_ballots)\n",
    "candidates = sorted(list(set([item for ranking in all_ballots for item in ranking])))\n",
    "num_cands = len(candidates)\n",
    "sample_weight = np.array([election[ballot] for ballot in all_ballots])\n",
    "\n",
    "#print(sample_weight)\n",
    "#print(num_ballots)\n",
    "#print(num_cands)\n",
    "#print(all_ballots)\n",
    "TOLERANCE = 14\n",
    "\n",
    "# preprocess data again\n",
    "distances_dict = {}\n",
    "possible_pairs = {}\n",
    "\n",
    "    \n",
    "for i in range(len(all_ballots)):\n",
    "    temp_list = []\n",
    "    #maximum = 0\n",
    "    #total = 0\n",
    "    for j in range(len(all_ballots)):\n",
    "        distances_dict[(i,j)] = HH_dist(all_ballots[i],all_ballots[j],num_cands, order = 1)\n",
    "        if True: #(distances_dict[(i,j)] <= TOLERANCE): # Not sure what tolerance should be, so ignoring. If it is too slow, consider adding some tolerance.\n",
    "            temp_list.append(j)\n",
    "        #maximum = max(maximum,distances_dict[(i,j)])\n",
    "        #total = total + distances_dict[(i,j)]\n",
    "    possible_pairs[i] = temp_list\n",
    "    #print(maximum,total/len(all_ballots))\n",
    "    #print(temp_list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00f4db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-median optimization model (with or with outliers)\n",
    "def formulate_new(possible_pairs, multiplicities,distances_dict, NUM_CLUSTERS, NUM_OUTLIERS=0):\n",
    "    m = gp.Model(\"clustering_new\")\n",
    "    m.Params.MIPGapAbs = 0.1 \n",
    "    pairs = {}\n",
    "    isCenter = {}\n",
    "    outlier = {}\n",
    "    for i in possible_pairs.keys():\n",
    "        isCenter[i] = m.addVar(vtype=GRB.BINARY, name = \"isCenter[%s]\" %i)\n",
    "        outlier[i] = m.addVar(vtype=GRB.BINARY, name = \"outlier[%s]\" %i)\n",
    "        for j in possible_pairs[i]:\n",
    "            pairs[i,j] = m.addVar(vtype=GRB.BINARY, name = \"pair{%s,%s}\" % (i,j))\n",
    "\n",
    "    for j in possible_pairs.keys():#constraint to define the isNotCenter variable\n",
    "        m.addConstr(isCenter[j] <= sum(pairs[i,j] for i in possible_pairs[j])) #isCenter[j] is LEQ than sum of all pairs [i,j]\n",
    "        for i in possible_pairs[j]:#to ensure that isCenter is 1 if any point has it as a center\n",
    "            m.addConstr(isCenter[j] - pairs[i,j] >= 0)\n",
    "\n",
    "    for i in possible_pairs.keys():\n",
    "        # this constraint was changed to ensure that each point i is either part of at least one cluster, or declared an outlier\n",
    "        m.addConstr(sum(pairs[i,j] for j in possible_pairs[i]) + outlier[i] >= 1) \n",
    "        m.addConstr(pairs[i,i] >= isCenter[i])\n",
    "\n",
    "    m.addConstr(sum(isCenter[j] for j in possible_pairs.keys()) == NUM_CLUSTERS)\n",
    "    m.addConstr(sum(multiplicities[i]*outlier[i] for i in possible_pairs.keys()) <= NUM_OUTLIERS)\n",
    "\n",
    "    m.setObjective(sum(multiplicities[i]*distances_dict[i,j]*pairs[i,j] for i in possible_pairs.keys() for j in possible_pairs[i]), GRB.MINIMIZE)\n",
    "    return m, pairs, isCenter, outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d382c611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2025-06-18\n",
      "Set parameter MIPGapAbs to value 0.1\n",
      "Gurobi Optimizer version 11.0.2 build v11.0.2rc0 (mac64[arm] - Darwin 24.1.0 24B5035e)\n",
      "\n",
      "CPU model: Apple M2 Max\n",
      "Thread count: 12 physical cores, 12 logical processors, using up to 12 threads\n",
      "\n",
      "Optimize a model with 1536360 rows, 1535120 columns and 6138004 nonzeros\n",
      "Model fingerprint: 0xb79e2089\n",
      "Variable types: 0 continuous, 1535120 integer (1535120 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+03]\n",
      "  Objective range  [5e-01, 2e+04]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+00]\n",
      "Presolve removed 3747 rows and 2508 columns (presolve time = 5s) ...\n",
      "Presolve removed 3747 rows and 2508 columns (presolve time = 10s) ...\n",
      "\n",
      "Interrupt request received\n",
      "Presolve removed 3747 rows and 2508 columns\n",
      "Presolve time: 10.24s\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 10.87 seconds (17.70 work units)\n",
      "Thread count was 1 (of 12 available processors)\n",
      "\n",
      "Solution count 0\n",
      "\n",
      "Solve interrupted\n",
      "Best objective -, best bound -, gap -\n"
     ]
    }
   ],
   "source": [
    "# Formulation without outliers\n",
    "m, pairs, isCenter, outlier = formulate_new(possible_pairs, sample_weight, distances_dict, NUM_CLUSTERS = 2)\n",
    "# Uncomment the line below to suppress Gurobi's output\n",
    "# m.Params.LogToConsole = 0\n",
    "m.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c75a7364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Solution:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Unable to retrieve attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m positive_centers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m m\u001b[38;5;241m.\u001b[39mgetVars():\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Check if the variable name matches the pattern \"IsCenter[i]\" and if its value is positive\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mvarName\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misCenter[\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m v\u001b[38;5;241m.\u001b[39mvarName\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m v\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     20\u001b[0m         positive_centers\u001b[38;5;241m.\u001b[39mappend(v)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m positive_centers:\n",
      "File \u001b[0;32msrc/gurobipy/var.pxi:125\u001b[0m, in \u001b[0;36mgurobipy.Var.__getattr__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/gurobipy/var.pxi:153\u001b[0m, in \u001b[0;36mgurobipy.Var.getAttr\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/gurobipy/attrutil.pxi:103\u001b[0m, in \u001b[0;36mgurobipy._getattr\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Unable to retrieve attribute 'x'"
     ]
    }
   ],
   "source": [
    "# Print optimal solution\n",
    "print(\"Optimal Solution:\")\n",
    "\n",
    "nearest = {}\n",
    "dim = int(num_cands*(num_cands-1)/2)\n",
    "\n",
    "positive_pairs = []\n",
    "for v in m.getVars():\n",
    "    # Check if the variable name matches the pattern \"IsCenter[i]\" and if its value is positive\n",
    "    if v.varName.startswith(\"pairs[\") and v.varName.endswith(\"]\") and v.x > 0:\n",
    "        positive_pairs.append(v)\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "positive_centers = []\n",
    "for v in m.getVars():\n",
    "    # Check if the variable name matches the pattern \"IsCenter[i]\" and if its value is positive\n",
    "    if v.varName.startswith(\"isCenter[\") and v.varName.endswith(\"]\") and v.x > 0:\n",
    "        positive_centers.append(v)\n",
    "        \n",
    "for v in positive_centers:\n",
    "    print(f\"{v.varName} = {v.x}\")\n",
    "    \n",
    "for v in positive_centers:\n",
    "    next_centroid = int(v.varName[9:-1])\n",
    "    print(next_centroid)\n",
    "    print(all_ballots[next_centroid])\n",
    "\n",
    "# double checking on objective function\n",
    "# print(\"Assignment costs\")\n",
    "for i in range(len(all_ballots)):\n",
    "    minimum = dim+1\n",
    "    for v in positive_centers:\n",
    "        next_centroid = int(v.varName[9:-1])\n",
    "        minimum = min(minimum,distances_dict[(i,next_centroid)])\n",
    "        if minimum == distances_dict[(i,next_centroid)]:\n",
    "            nearest[i] = next_centroid\n",
    "    print(i,minimum)\n",
    "   \n",
    "#medians = [v.index for v in m.getVars() if (v.varName.startswith(\"isCenter[\") and v.x == 1) ]#and v.varName.startswith(\"isCenter[\")\n",
    "#print(medians)\n",
    "#for i in medians\n",
    "    #print(all_ballots[i])\n",
    "                                            \n",
    "\n",
    "#print(\"Ballots with IsCenter[i] == 1:\")\n",
    "#for i in range(len(all_ballots)):\n",
    "#    if IsCenter[i].x == 1:\n",
    "#        print(f\"{all_ballots[i]} is a median.\")\n",
    "\n",
    "print(f\"Optimal Objective Value: {m.objVal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf7390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up for Borda with L_2 squared metric\n",
    "election, cand_names, location = parse('data/edinburgh17-02.blt')\n",
    "#num_cands = max([item for ranking in election.keys() for item in ranking])\n",
    "\n",
    "all_ballots = list(election.keys())\n",
    "num_ballots = len(all_ballots)\n",
    "candidates = sorted(list(set([item for ranking in all_ballots for item in ranking])))\n",
    "num_cands = len(candidates)\n",
    "sample_weight = np.array([election[ballot] for ballot in all_ballots])\n",
    "\n",
    "#print(sample_weight)\n",
    "#print(num_ballots)\n",
    "#print(num_cands)\n",
    "#print(all_ballots)\n",
    "TOLERANCE = 14\n",
    "\n",
    "# preprocess data again\n",
    "distances_dict = {}\n",
    "possible_pairs = {}\n",
    "\n",
    "    \n",
    "for i in range(len(all_ballots)):\n",
    "    temp_list = []\n",
    "    #maximum = 0\n",
    "    #total = 0\n",
    "    for j in range(len(all_ballots)):\n",
    "        distances_dict[(i,j)] = Borda_dist(all_ballots[i],all_ballots[j],num_cands, borda_style='bord', order = 2)**2\n",
    "        if True:#(distances_dict[(i,j)] <= TOLERANCE): # Not sure what tolerance should be, so ignoring. If it is too slow, consider adding some tolerance.\n",
    "            temp_list.append(j)\n",
    "        #maximum = max(maximum,distances_dict[(i,j)])\n",
    "        #total = total + distances_dict[(i,j)]\n",
    "    possible_pairs[i] = temp_list\n",
    "    #print(maximum,total/len(all_ballots))\n",
    "    #print(temp_list)\n",
    "    \n",
    "# X = np.array([HH_proxy(ballot,num_cands=num_cands) for ballot in all_ballots])\n",
    "\n",
    "# model = KMeans(n_clusters=k, n_init=n_init).fit(X,sample_weight=sample_weight)\n",
    "# labels = model.labels_\n",
    "# centroids = model.cluster_centers_\n",
    "    \n",
    "# C = [dict() for _ in range(k)]\n",
    "# for count in range(len(all_ballots)):\n",
    "#     ballot = all_ballots[count]\n",
    "#     C[labels[count]][ballot]=election[ballot]\n",
    "# if return_centroids:\n",
    "#     return C, centroids\n",
    "# else:\n",
    "#     return C\n",
    "    \n",
    "# def Manhattan_dist(A,B):\n",
    "#     return sum(np.abs(A-B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b4c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up for Borda with L_1 metric\n",
    "election, cand_names, location = parse('data/edinburgh17-02.blt')\n",
    "#num_cands = max([item for ranking in election.keys() for item in ranking])\n",
    "\n",
    "all_ballots = list(election.keys())\n",
    "num_ballots = len(all_ballots)\n",
    "candidates = sorted(list(set([item for ranking in all_ballots for item in ranking])))\n",
    "num_cands = len(candidates)\n",
    "sample_weight = np.array([election[ballot] for ballot in all_ballots])\n",
    "\n",
    "#print(sample_weight)\n",
    "#print(num_ballots)\n",
    "#print(num_cands)\n",
    "#print(all_ballots)\n",
    "TOLERANCE = 14\n",
    "\n",
    "# preprocess data again\n",
    "distances_dict = {}\n",
    "possible_pairs = {}\n",
    "\n",
    "    \n",
    "for i in range(len(all_ballots)):\n",
    "    temp_list = []\n",
    "    #maximum = 0\n",
    "    #total = 0\n",
    "    for j in range(len(all_ballots)):\n",
    "        distances_dict[(i,j)] = Borda_dist(all_ballots[i],all_ballots[j],num_cands, borda_style='bord', order = 1)\n",
    "        if True:#(distances_dict[(i,j)] <= TOLERANCE): # Not sure what tolerance should be, so ignoring. If it is too slow, consider adding some tolerance.\n",
    "            temp_list.append(j)\n",
    "        #maximum = max(maximum,distances_dict[(i,j)])\n",
    "        #total = total + distances_dict[(i,j)]\n",
    "    possible_pairs[i] = temp_list\n",
    "    #print(maximum,total/len(all_ballots))\n",
    "    #print(temp_list)\n",
    "    \n",
    "# X = np.array([HH_proxy(ballot,num_cands=num_cands) for ballot in all_ballots])\n",
    "\n",
    "# model = KMeans(n_clusters=k, n_init=n_init).fit(X,sample_weight=sample_weight)\n",
    "# labels = model.labels_\n",
    "# centroids = model.cluster_centers_\n",
    "    \n",
    "# C = [dict() for _ in range(k)]\n",
    "# for count in range(len(all_ballots)):\n",
    "#     ballot = all_ballots[count]\n",
    "#     C[labels[count]][ballot]=election[ballot]\n",
    "# if return_centroids:\n",
    "#     return C, centroids\n",
    "# else:\n",
    "#     return C\n",
    "    \n",
    "# def Manhattan_dist(A,B):\n",
    "#     return sum(np.abs(A-B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01601250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizing for Borda with L_1 metric\n",
    "m, pairs, isCenter, outlier = formulate_new(possible_pairs, sample_weight, distances_dict, NUM_CLUSTERS = 2)\n",
    "    # Uncomment the line below to suppress Gurobi's output\n",
    "    # m.Params.LogToConsole = 0\n",
    "m.optimize()\n",
    "\n",
    "\n",
    "# Print optimal solution\n",
    "print(\"Optimal Solution:\")\n",
    "\n",
    "positive_pairs = []\n",
    "for v in m.getVars():\n",
    "    # Check if the variable name matches the pattern \"IsCenter[i]\" and if its value is positive\n",
    "    if v.varName.startswith(\"pairs[\") and v.varName.endswith(\"]\") and v.x > 0:\n",
    "        positive_pairs.append(v)      \n",
    "\n",
    "positive_centers = []\n",
    "outliers = []\n",
    "for v in m.getVars():\n",
    "    # Check if the variable name matches the pattern \"IsCenter[i]\" and if its value is positive\n",
    "    if v.varName.startswith(\"isCenter[\") and v.varName.endswith(\"]\") and v.x > 0:\n",
    "        positive_centers.append(v)\n",
    "    if v.varName.startswith(\"outlier[\") and v.varName.endswith(\"]\") and v.x > 0:\n",
    "            outliers.append(v)\n",
    "    \n",
    "        \n",
    "for v in positive_centers:\n",
    "    print(f\"{v.varName} = {v.x}\")\n",
    "    \n",
    "for v in positive_centers:\n",
    "    next_centroid = int(v.varName[9:-1])\n",
    "    #print(next_centroid)\n",
    "    print(all_ballots[next_centroid])\n",
    "\n",
    "print(\"Assignment costs\")\n",
    "total = 0\n",
    "counts = {}\n",
    "for i in range(len(all_ballots)):\n",
    "     counts[i] = sample_weight[i]\n",
    "for v in outliers:\n",
    "     next_outlier = int(v.varName[8:-1])\n",
    "     counts[next_outlier] = 0\n",
    "\n",
    "for i in range(len(all_ballots)):\n",
    "    minimum = 30\n",
    "    for v in positive_centers:\n",
    "        next_centroid = int(v.varName[9:-1])\n",
    "        minimum = min(minimum,distances_dict[(i,next_centroid)])\n",
    "    total = total + minimum*counts[i]\n",
    "print(total)\n",
    "\n",
    "print(f\"Optimal Objective Value: {m.objVal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e532f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing for continuous k-median with HH_metric \n",
    "all_ballots = list(election.keys())\n",
    "num_ballots = len(all_ballots)\n",
    "candidates = sorted(list(set([item for ranking in all_ballots for item in ranking])))\n",
    "num_cands = len(candidates)\n",
    "sample_weight = np.array([election[ballot] for ballot in all_ballots])\n",
    "X = np.array([HH_proxy(ballot,num_cands=num_cands) for ballot in all_ballots])\n",
    "NUM_CLUSTERS = 2\n",
    "bigMone = []\n",
    "#bigminusoneM = []\n",
    "num_votes = sum(sample_weight[i] for i in range(num_ballots))\n",
    "test_index = math.floor(len(all_ballots)/5)\n",
    "dim = int(num_cands*(num_cands-1)/2)\n",
    "\n",
    "bigMone = {\n",
    "    j: sum(sample_weight[i] for i in range(num_ballots) if X[i][j] > 0)\n",
    "    for j in range(dim)\n",
    "}\n",
    "\n",
    "for j in range(dim):\n",
    "    print(f\"bigMone[{j}] = {bigMone[j]}\")\n",
    "    \n",
    "bigMminusone = {\n",
    "    j: sum(sample_weight[i] for i in range(num_ballots) if X[i][j] < 0)\n",
    "    for j in range(dim)\n",
    "}\n",
    "\n",
    "for j in range(dim):\n",
    "    print(f\"bigMminusone[{j}] = {bigMminusone[j]}\")\n",
    "\n",
    "#for j in range(dim):\n",
    "#     bigMone[j] = sum(sample_weight[i] for i in range(num_ballots)) #if X[i][j] > 0)\n",
    "#     bigminusoneM[j] == sum(sample_weight[i] for i in range(num_ballots) if X[i][j] < 0)\n",
    "\n",
    "LB = sum(min(bigMone[j],bigMminusone[j],num_votes - bigMone[j] -bigMminusone[j]) for j in range(dim))\n",
    "\n",
    "print(test_index)\n",
    "print(sample_weight[test_index])\n",
    "print(all_ballots[test_index])\n",
    "print(X[test_index])\n",
    "print(dim)\n",
    "print(num_votes)\n",
    "print(bigMone)\n",
    "print(LB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe9e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous k-median integer programming L_1 optimization model (without outliers) for HH_metric\n",
    "#starting point for Grant\n",
    "def formulate_medians(num_ballots,num_votes,multiplicities,dimension,bigMplus,bigMminus,X,NUM_CLUSTERS=2):\n",
    "    m = gp.Model(\"continuous_HH_k_medians\")\n",
    "    #m.Params.MIPGapAbs = 0.1 \n",
    "    \n",
    "    assignment = {}\n",
    "    weight_one = {}\n",
    "    weight_zero = {}\n",
    "    weight_minusone = {}\n",
    "    one_cost = {}\n",
    "    one_costis0 = {}\n",
    "    minusone_cost = {}\n",
    "    minusone_costis0 ={}\n",
    "    median_one = {}\n",
    "    median_zero = {}\n",
    "    median_minusone = {}\n",
    "    median_value = {}\n",
    "\n",
    "    \n",
    "    for i in range(num_ballots):\n",
    "        for c in range(NUM_CLUSTERS):\n",
    "            assignment[i,c] = m.addVar(vtype=GRB.BINARY, name = \"assignment{%s,%s}\" % (i,c))\n",
    "        \n",
    "    for j in range(dimension):\n",
    "        for c in range(NUM_CLUSTERS):\n",
    "            weight_one[j,c] = m.addVar(lb=0,vtype=GRB.INTEGER, name = \"weight_one{%s,%s}\" % (j,c))\n",
    "            weight_zero[j,c] = m.addVar(lb=0,vtype=GRB.INTEGER, name = \"weight_zero{%s,%s}\" % (j,c))\n",
    "            weight_minusone[j,c] = m.addVar(lb=0,vtype=GRB.INTEGER, name = \"weight_negone{%s,%s}\" % (j,c))\n",
    "            one_cost[j,c] = m.addVar(lb=0,vtype=GRB.INTEGER, name = \"one_cost{%s,%s}\" % (j,c))\n",
    "            minusone_cost[j,c] = m.addVar(lb=0,vtype=GRB.INTEGER, name = \"minusone_cost{%s,%s}\" % (j,c))\n",
    "            median_one[j,c] = m.addVar(vtype=GRB.BINARY, name = \"median_one{%s,%s}\" % (j,c))\n",
    "            median_zero[j,c] = m.addVar(vtype=GRB.BINARY, name = \"median_zero{%s,%s}\" % (j,c))\n",
    "            median_minusone[j,c] = m.addVar(vtype=GRB.BINARY, name = \"median_minusone{%s,%s}\" % (j,c))\n",
    "            median_value[j,c] = m.addVar(lb=-1,vtype=GRB.INTEGER, name = \"median_value{%s,%s}\" % (j,c))\n",
    "            \n",
    "            \n",
    "    for i in range(num_ballots):\n",
    "        m.addConstr(1 == sum(assignment[i,c] for c in range(NUM_CLUSTERS)))\n",
    "    \n",
    "    for c in range(NUM_CLUSTERS-1):\n",
    "        m.addConstr(sum(assignment[i,c] for i in range(num_ballots))<= sum(assignment[i,c+1] for i in range(num_ballots)))\n",
    "    \n",
    "    for j in range(dimension):\n",
    "        for c in range(NUM_CLUSTERS):\n",
    "            # for each dimension, and each cluster, exactly one value is selected as the median value\n",
    "            m.addConstr(1 == median_one[j,c] + median_zero[j,c] + median_minusone[j,c])\n",
    "\n",
    "            # and these imply the actual median value\n",
    "            m.addConstr(median_value[j,c] == median_one[j,c]-median_minusone[j,c])\n",
    "            \n",
    "            # for each dimension, and each cluster, count the number assigned for each possible median value\n",
    "            m.addConstr(weight_one[j,c] == sum(multiplicities[i]*assignment[i,c] for i in range(num_ballots) if X[i][j] > 0))\n",
    "            m.addConstr(weight_zero[j,c] == sum(multiplicities[i]*assignment[i,c] for i in range(num_ballots) if X[i][j] == 0))\n",
    "            m.addConstr(weight_minusone[j,c] == sum(multiplicities[i]*assignment[i,c] for i in range(num_ballots) if X[i][j] < 0))\n",
    "            \n",
    "            #these constraints force that one_cost[j,c] == max{weight_one[j,c]-weight_zero[j,c]-weight_minusone[j,c],0}\n",
    "            m.addConstr(one_cost[j,c] >= weight_one[j,c]-weight_zero[j,c]-weight_minusone[j,c])\n",
    "            m.addConstr(one_cost[j,c] <= bigMplus[j]*(median_one[j,c]))\n",
    "            #m.addConstr(one_cost[j,c] <= num_votes*(1-one_costis0[j,c]))\n",
    "            m.addConstr(one_cost[j,c] <= weight_one[j,c]-weight_zero[j,c]-weight_minusone[j,c] + (num_votes-bigMplus[j])*(1-median_one[j,c]))\n",
    "            #m.addConstr(one_cost[j,c] <= weight_one[j,c]-weight_zero[j,c]-weight_minusone[j,c] + num_votes*one_costis0[j,c])\n",
    "            \n",
    "            #these constraints force that minusone_cost[j,c] == max{weight_minusone[j,c]-weight_zero[j,c]-weight_one[j,c],0}\n",
    "            m.addConstr(minusone_cost[j,c] >= weight_minusone[j,c]-weight_zero[j,c]-weight_one[j,c])\n",
    "            m.addConstr(minusone_cost[j,c] <= bigMminus[j]*median_minusone[j,c])\n",
    "            #m.addConstr(minusone_cost[j,c] <= num_votes*(1-minusone_costis0[j,c]))\n",
    "            m.addConstr(minusone_cost[j,c] <= weight_minusone[j,c]-weight_zero[j,c]-weight_one[j,c] + (num_votes-bigMminus[j])*(1-median_minusone[j,c]))\n",
    "            #m.addConstr(minusone_cost[j,c] <= weight_minusone[j,c]-weight_zero[j,c]-weight_one[j,c] + num_votes*minusone_costis0[j,c])\n",
    "            \n",
    "            #add constraints that also ensure that median_zero is = 1 only when the counts indicate\n",
    "            \n",
    "    #m.addConstr(sum(weight_one[j,c]+weight_minusone[j,c]-one_cost[j,c]-minusone_cost[j,c] for j in range(dimension) for c in range(NUM_CLUSTERS)) >= lowerbound)\n",
    "    \n",
    "    #m.setObjective(sum(weight_one[j,c]+weight_zero[j,c]+weight_minusone[j,c] for j in range(dimension) for c in range(NUM_CLUSTERS)), GRB.MINIMIZE)\n",
    "    #m.setObjective(sum(assignment[i,c]*multiplicities[i] for i in range(num_ballots) for c in range(NUM_CLUSTERS)), GRB.MINIMIZE)\n",
    "    m.setObjective(sum(weight_one[j,c]+weight_minusone[j,c]-one_cost[j,c]-minusone_cost[j,c] for j in range(dimension) for c in range(NUM_CLUSTERS)), GRB.MINIMIZE)\n",
    "    return m, assignment, weight_one, weight_zero, weight_minusone, median_one, median_zero, median_minusone, median_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16126922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running continuous k-medians\n",
    "m, clusters, ones, zeros, minusones, median1, median0, medianm1, median = formulate_medians(num_ballots,num_votes,sample_weight,dim,bigMone,bigMminusone,X,NUM_CLUSTERS)\n",
    "    # Uncomment the line below to suppress Gurobi's output\n",
    "    # m.Params.LogToConsole = 0\n",
    "m.optimize()\n",
    "\n",
    "\n",
    "#Print optimal solution\n",
    "print(\"Optimal Solution:\") \n",
    "\n",
    "assigned = []\n",
    "one = []\n",
    "minusone = []\n",
    "zero = []\n",
    "object_terms = []\n",
    "#median_coordinates = []\n",
    "\n",
    "\n",
    "# for v in m.getVars():\n",
    "#     print(f\"{v.varName}={v.x}\")\n",
    "\n",
    "\n",
    "# Check if the optimization was successful\n",
    "if m.status == gp.GRB.OPTIMAL:\n",
    "    # Iterate over the variables to find median_value\n",
    "    for c in range(NUM_CLUSTERS):\n",
    "        for j in range(dim):\n",
    "            # var_name1 = \"weight_one{%s,%s}\" % (j, c)\n",
    "            # var1 = m.getVarByName(var_name1)\n",
    "            # if var1 is not None:\n",
    "            #     print(f\"Optimal value of {var_name1}: {var1.x}\")\n",
    "            # var_name2 = \"weight_zero{%s,%s}\" % (j, c)\n",
    "            # var2 = m.getVarByName(var_name2)\n",
    "            # if var2 is not None:\n",
    "            #     print(f\"Optimal value of {var_name2}: {var2.x}\")\n",
    "            # var_name3 = \"weight_negone{%s,%s}\" % (j, c)\n",
    "            # var3 = m.getVarByName(var_name3)\n",
    "            # if var3 is not None:\n",
    "            #     print(f\"Optimal value of {var_name3}: {var3.x}\")\n",
    "            # var_name1 = \"median_one{%s,%s}\" % (j, c)\n",
    "            # var1 = m.getVarByName(var_name1)\n",
    "            # if var1 is not None:\n",
    "            #     print(f\"Optimal value of {var_name1}: {var1.x}\")\n",
    "            # var_name2 = \"median_zero{%s,%s}\" % (j, c)\n",
    "            # var2 = m.getVarByName(var_name2)\n",
    "            # if var2 is not None:\n",
    "            #     print(f\"Optimal value of {var_name2}: {var2.x}\")\n",
    "            # var_name3 = \"median_minusone{%s,%s}\" % (j, c)\n",
    "            # var3 = m.getVarByName(var_name3)\n",
    "            # if var3 is not None:\n",
    "            #     print(f\"Optimal value of {var_name3}: {var3.x}\")\n",
    "            var_name4 = \"median_value{%s,%s}\" % (j, c)\n",
    "            var4 = m.getVarByName(var_name4)\n",
    "            if var4 is not None:\n",
    "                print(f\"Optimal value of {var_name4}: {var4.x}\")\n",
    "    \n",
    "\n",
    "#     if v.varName.startswith(\"clusters[\") and v.varName.endswith(\"]\"): #and v.x > 0:\n",
    "#         assigned.append(v)\n",
    "#if v.varName.startswith(\"median_one[\") and v.varName.endswith(\"]\"):\n",
    "#    one.append(v)\n",
    "#if v.varName.startswith(\"median_zero[\") and v.varName.endswith(\"]\"):\n",
    "#    zero.append(v)\n",
    "#if v.varName.startswith(\"median_minusone[\") and v.varName.endswith(\"]\"):\n",
    "#    minusone.append(v)\n",
    "#     if v.varName.startswith(\"one_terms[\") and v.varName.endswith(\"]\"):\n",
    "#         object_terms.append(v)\n",
    "        \n",
    "# for v in assigned:\n",
    "#     print(f\"{v.varName} = {v.x}\")\n",
    "    \n",
    "# for v in one:\n",
    "#      print(f\"{v.varName} = {v.x}\")\n",
    "\n",
    "    \n",
    "# for v in zero:\n",
    "#      print(f\"{v.varName} = {v.x}\")\n",
    "\n",
    "    \n",
    "# for v in minusone:\n",
    "#      print(f\"{v.varName} = {v.x}\")\n",
    "    \n",
    "# for v in object_terms:\n",
    "#     print(f\"{v.varName} = {v.x}\")\n",
    "    \n",
    "# for v in positive_centers:\n",
    "#     next_centroid = int(v.varName[9:-1])\n",
    "#     #print(next_centroid)\n",
    "#     print(all_ballots[next_centroid])\n",
    "\n",
    "# print(\"Assignment costs\")\n",
    "# total = 0\n",
    "# counts = {}\n",
    "# for i in range(len(all_ballots)):\n",
    "#      counts[i] = sample_weight[i]\n",
    "# for v in outliers:\n",
    "#      next_outlier = int(v.varName[8:-1])\n",
    "#      counts[next_outlier] = 0\n",
    "\n",
    "# for i in range(len(all_ballots)):\n",
    "#     minimum = 30\n",
    "#     for v in positive_centers:\n",
    "#         next_centroid = int(v.varName[9:-1])\n",
    "#         minimum = min(minimum,distances_dict[(i,next_centroid)])\n",
    "#     total = total + minimum*counts[i]\n",
    "# print(total)\n",
    "\n",
    "print(f\"Optimal Objective Value: {m.objVal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d17dd32b-48e2-4199-8acd-7a0104970785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1238, 21)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1926b8ab-4ab3-4db0-8f41-9929a137b188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1238,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a38d7a7-d721-4bb2-b10a-71db9142d369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2eacd8-dad4-417b-a2e9-2fb8d615e7df",
   "metadata": {},
   "source": [
    "# Continuous K-Median for All Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25d88d9e-83fe-4ba1-b30b-c64b2ca779a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_continuous_model(ballots, weights, num_clusters, value_set):\n",
    "    num_ballots, num_candidates = ballots.shape\n",
    "    assert weights.shape[0] == num_ballots, \"Weights and ballots dimensions do not match.\"\n",
    "    D = range(num_ballots)\n",
    "    K = range(num_clusters)\n",
    "    V = value_set\n",
    "    Dims = range(num_candidates)\n",
    "    model = gp.Model(\"continuous_k_medians\")\n",
    "\n",
    "    # x[j,r]: 1 if ballot j is assigned to cluster r\n",
    "    x = {(j,r): model.addVar(vtype=GRB.BINARY, name=f\"x[{j},{r}]\") \n",
    "         for j in D for r in K}\n",
    "    \n",
    "    # z[i,r,v]: 1 if ith dimension of the median of the rth cluster is v\n",
    "    z = {(i,r,v): model.addVar(vtype=GRB.BINARY, name=f\"z[{i},{r},{v}]\") \n",
    "        for i in Dims for r in K for v in V}\n",
    "\n",
    "    L = {(i,r,v): model.addVar(vtype=GRB.BINARY, name=f\"L[{i},{r},{v}]\") \n",
    "        for i in Dims for r in K for v in V}\n",
    "\n",
    "    R = {(i,r,v): model.addVar(vtype=GRB.BINARY, name=f\"R[{i},{r},{v}]\") \n",
    "        for i in Dims for r in K for v in V}\n",
    "\n",
    "    # W[i,r,v]: total weight of points in D assigned to cluster r with coordinate i of value v\n",
    "    W = {}\n",
    "    for i in Dims:\n",
    "        for r in K:\n",
    "            for v in value_set:\n",
    "                W[i,r,v] = gp.quicksum([weights[j]*x[j,r] for j in D if ballots[j, i] == v])\n",
    "                \n",
    "    # C[i,r]: contribution of coordinate i to cost of cluster r\n",
    "    C = {(i,r): model.addVar(vtype=GRB.CONTINUOUS, name=f\"C[{i},{r}]\")\n",
    "        for i in Dims for r in K}\n",
    "    \n",
    "    # Define \"big-M\"\n",
    "    M = np.sum(weights)\n",
    "    \n",
    "    # McCormick Constraints (z = LR)\n",
    "    for i in Dims:\n",
    "        for r in K:\n",
    "            for v in V:\n",
    "                model.addLConstr(z[i,r,v] - L[i,r,v] <= 0)\n",
    "                model.addLConstr(z[i,r,v] - R[i,r,v] <= 0)\n",
    "                model.addLConstr(L[i,r,v] + R[i,r,v] - z[i,r,v] <= 1)\n",
    "    \n",
    "    for j in D:\n",
    "        # Constraint: Each ballot j is assigned to exactly one cluster r\n",
    "        model.addLConstr(gp.quicksum([x[j,r] for r in K]) == 1)\n",
    "    \n",
    "    \n",
    "    for i in Dims:\n",
    "        for r in K:\n",
    "            # Constraint: There is exactly one median value for each cluster and coordinate\n",
    "            model.addLConstr(gp.quicksum(z[i,r,v] for v in V) == 1)            \n",
    "            for v_target in V:\n",
    "                # Constraint: Median finding constraints, both L and R have to be 1 for z to be 1 (from McCormick)\n",
    "                model.addLConstr(gp.quicksum([W[i,r,v] if v > v_target else -1*W[i,r,v] for v in V]) - M*(1-L[i,r,v]) <= 0)\n",
    "                model.addLConstr(gp.quicksum([W[i,r,v] if v < v_target else -1*W[i,r,v] for v in V]) - M*(1-R[i,r,v]) <= 0)\n",
    "\n",
    "                # Constraint: Bound C\n",
    "                model.addLConstr(C[i,r] - gp.quicksum([np.abs(v - v_target)*W[i,r,v] for v in V]) <= 0)\n",
    "                model.addLConstr(gp.quicksum([np.abs(v - v_target)*W[i,r,v] for v in V]) - C[i,r] - M*(1-z[i,r,v_target]) <= 0)\n",
    "\n",
    "    model.setObjective(gp.quicksum([C[i,r] for i in Dims for r in K]), GRB.MINIMIZE)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6173e8f3-5b00-45f3-8168-0dc0cea8f78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa9391-b9a2-4ed5-8696-ff0c22f1bd52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02849074-dc04-428e-83f7-09da1d238431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_continuous_model(ballots, weights, num_clusters, value_set):\n",
    "    num_ballots, num_cand_choose_2 = ballots.shape\n",
    "    assert weights.shape[0] == num_ballots, \"Weights and ballots dimensions do not match.\"\n",
    "    D = range(num_ballots)\n",
    "    K = range(num_clusters)\n",
    "    V = value_set\n",
    "    Dims = range(num_cand_choose_2)\n",
    "    model = gp.Model(\"continuous_k_medians\")\n",
    "\n",
    "    # x[j,r]: 1 if ballot j is assigned to cluster r\n",
    "    x = {(j,r): model.addVar(vtype=GRB.BINARY, name=f\"x[{j},{r}]\") \n",
    "         for j in D for r in K}\n",
    "    \n",
    "    # z[i,r,v]: 1 if ith dimension of the median of the rth cluster is v\n",
    "    z = {(i,r,v): model.addVar(vtype=GRB.BINARY, name=f\"z[{i},{r},{v}]\") \n",
    "        for i in Dims for r in K for v in V}\n",
    "\n",
    "    # W[i,r,v]: total weight of points in D assigned to cluster r with coordinate i of value v\n",
    "    W = {(i,r,v): model.addVar(vtype=GRB.INTEGER, name=f\"W[{i},{r},{v}]\")\n",
    "         for i in Dims for r in K for v in V}\n",
    "                \n",
    "    # C[i,r]: contribution of coordinate i to cost of cluster r\n",
    "    # TODO: Integer or continuous?\n",
    "    C = {(i,r): model.addVar(vtype=GRB.INTEGER, name=f\"C[{i},{r}]\")\n",
    "        for i in Dims for r in K}\n",
    "    \n",
    "    # Define \"big-M\"\n",
    "    M = np.sum(weights)\n",
    "    \n",
    "    for j in D:\n",
    "        # Constraint: Each ballot j is assigned to exactly one cluster r\n",
    "        model.addLConstr(gp.quicksum([x[j,r] for r in K]) == 1)\n",
    "\n",
    "    # break symmetry to have cluster r-1 have fewer distinct ballots than cluster r\n",
    "    for r in range(num_clusters-1):\n",
    "        model.addLConstr(sum(x[i,r] for i in range(num_ballots))<= sum(x[i,r+1] for i in range(num_ballots)))\n",
    "\n",
    "    for i in Dims:\n",
    "        for r in K:\n",
    "            for v in value_set:\n",
    "                model.addLConstr(gp.quicksum([weights[j]*x[j,r] for j in D if ballots[j, i] == v]) - W[i,r,v] == 0)\n",
    "                \n",
    "    \n",
    "    for i in Dims:\n",
    "        for r in K:\n",
    "            # Constraint: There is exactly one median value for each cluster and coordinate\n",
    "            model.addLConstr(gp.quicksum(z[i,r,v] for v in V) == 1)            \n",
    "            for v_target in V:\n",
    "                # Constraint: Median finding constraints\n",
    "                model.addLConstr(gp.quicksum([W[i,r,v] if v > v_target else -1*W[i,r,v] for v in V]) - M*(1-z[i,r,v_target]) <= 0)\n",
    "                model.addLConstr(gp.quicksum([W[i,r,v] if v < v_target else -1*W[i,r,v] for v in V]) - M*(1-z[i,r,v_target]) <= 0)\n",
    "\n",
    "                # Constraint: Bound C\n",
    "                model.addLConstr(C[i,r] - gp.quicksum([np.abs(v - v_target)*W[i,r,v] for v in V]) <= 0)\n",
    "                model.addLConstr(gp.quicksum([np.abs(v - v_target)*W[i,r,v] for v in V]) - C[i,r] - M*(1-z[i,r,v_target]) <= 0)\n",
    "\n",
    "    model.setObjective(gp.quicksum([C[i,r] for i in Dims for r in K]), GRB.MINIMIZE)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "108b6d41-7ebe-4bda-a7a9-48666ac7c8ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(X \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, X)\n\u001b[1;32m      2\u001b[0m X_1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(X_1 \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1\u001b[39m, X_1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "X_1 = np.where(X == -0.5, -1, X)\n",
    "X_1 = np.where(X_1 == 0.5, 1, X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c5c63-a1c8-4901-b4f5-841774975ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_continuous_model(X_1, sample_weight, 2, np.array([-1,0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1cf7913-aaa8-477d-af3f-96321ba9880f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_weight' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample_weight\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_weight' is not defined"
     ]
    }
   ],
   "source": [
    "sample_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca916135-44e6-4ff4-8160-fa36058653fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39moptimize()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "624a2941-c7d2-4982-9d04-77d9f8b1aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ballots = X\n",
    "weights = sample_weight\n",
    "num_clusters = 2\n",
    "value_set = np.array([-1,0,1])\n",
    "num_ballots, num_candidates = ballots.shape\n",
    "assert weights.shape[0] == num_ballots, \"Weights and ballots dimensions do not match.\"\n",
    "D = range(num_ballots)\n",
    "K = range(num_clusters)\n",
    "V = value_set\n",
    "Dims = range(num_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f5efacfb-ddec-438a-b9b2-2e942d53bb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median at 0,0 = 0\n",
      "Median at 1,0 = 0\n",
      "Median at 2,0 = 0\n",
      "Median at 3,0 = 0\n",
      "Median at 4,0 = 0\n",
      "Median at 5,0 = 0\n",
      "Median at 6,0 = 0\n",
      "Median at 7,0 = 0\n",
      "Median at 8,0 = 0\n",
      "Median at 9,0 = 0\n",
      "Median at 10,0 = 0\n",
      "Median at 11,0 = 0\n",
      "Median at 12,0 = 0\n",
      "Median at 13,0 = 0\n",
      "Median at 14,0 = 0\n",
      "Median at 15,0 = 0\n",
      "Median at 16,0 = 0\n",
      "Median at 17,0 = 0\n",
      "Median at 18,0 = 0\n",
      "Median at 19,0 = 0\n",
      "Median at 20,0 = 0\n",
      "Median at 0,1 = 0\n",
      "Median at 1,1 = 0\n",
      "Median at 2,1 = 0\n",
      "Median at 3,1 = 0\n",
      "Median at 4,1 = 0\n",
      "Median at 5,1 = -1\n",
      "Median at 6,1 = 0\n",
      "Median at 7,1 = 0\n",
      "Median at 8,1 = 0\n",
      "Median at 9,1 = 0\n",
      "Median at 10,1 = -1\n",
      "Median at 11,1 = -1\n",
      "Median at 12,1 = 0\n",
      "Median at 13,1 = 0\n",
      "Median at 14,1 = -1\n",
      "Median at 15,1 = 0\n",
      "Median at 16,1 = 0\n",
      "Median at 17,1 = -1\n",
      "Median at 18,1 = 0\n",
      "Median at 19,1 = -1\n",
      "Median at 20,1 = -1\n"
     ]
    }
   ],
   "source": [
    "for r in K:\n",
    "    for i in Dims:\n",
    "        value = -1\n",
    "        for v in V:\n",
    "            if model.getVarByName(f\"z[{i},{r},{v}]\").X == 1:\n",
    "                value = v\n",
    "        print(f\"Median at {i},{r} = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6906a527-6207-4812-b552-dfe8d0d2fd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['filename', 'n_clusters', 'outlier_fraction',\n",
      "       'positive_centers (idx, x, ballot)', 'optimal value',\n",
      "       'distance function', 'median_dict'],\n",
      "      dtype='object')\n",
      "                         filename  n_clusters  outlier_fraction  \\\n",
      "0         aberdeen_2012_ward1.csv           2               0.0   \n",
      "1         aberdeen_2012_ward1.csv           2               0.0   \n",
      "2        aberdeen_2012_ward10.csv           2               0.0   \n",
      "3        aberdeen_2012_ward10.csv           2               0.0   \n",
      "4        aberdeen_2012_ward11.csv           2               0.0   \n",
      "...                           ...         ...               ...   \n",
      "2135  west_lothian_2022_ward7.csv           2               0.0   \n",
      "2136  west_lothian_2022_ward8.csv           2               0.0   \n",
      "2137  west_lothian_2022_ward8.csv           2               0.0   \n",
      "2138  west_lothian_2022_ward9.csv           2               0.0   \n",
      "2139  west_lothian_2022_ward9.csv           2               0.0   \n",
      "\n",
      "                positive_centers (idx, x, ballot)  optimal value  \\\n",
      "0      {(21, 1.0, (1, 3)), (627, 1.0, (5, 4, 8))}        27269.5   \n",
      "1      {(21, 1.0, (1, 3)), (627, 1.0, (5, 4, 8))}        53739.0   \n",
      "2            {(0, 1.0, (1,)), (806, 1.0, (7, 4))}        33386.0   \n",
      "3         {(10, 1.0, (1, 3)), (806, 1.0, (7, 4))}        65754.0   \n",
      "4        {(263, 1.0, (4, 5)), (448, 1.0, (6, 1))}        16843.0   \n",
      "...                                           ...            ...   \n",
      "2135     {(733, 1.0, (5, 7)), (317, 1.0, (1, 2))}        47764.0   \n",
      "2136    {(226, 1.0, (2, 7)), (1077, 1.0, (1, 8))}        36756.5   \n",
      "2137    {(226, 1.0, (2, 7)), (1077, 1.0, (1, 8))}        72795.0   \n",
      "2138  {(344, 1.0, (4, 1, 2)), (585, 1.0, (1, 6))}        22865.5   \n",
      "2139  {(344, 1.0, (4, 1, 2)), (585, 1.0, (1, 6))}        44527.0   \n",
      "\n",
      "     distance function                median_dict  \n",
      "0              HH_dist  {0: (1, 3), 1: (5, 4, 8)}  \n",
      "1           Borda_dist  {0: (1, 3), 1: (5, 4, 8)}  \n",
      "2              HH_dist       {0: (1,), 1: (7, 4)}  \n",
      "3           Borda_dist     {0: (1, 3), 1: (7, 4)}  \n",
      "4              HH_dist     {0: (4, 5), 1: (6, 1)}  \n",
      "...                ...                        ...  \n",
      "2135        Borda_dist     {0: (5, 7), 1: (1, 2)}  \n",
      "2136           HH_dist     {0: (2, 7), 1: (1, 8)}  \n",
      "2137        Borda_dist     {0: (2, 7), 1: (1, 8)}  \n",
      "2138           HH_dist  {0: (4, 1, 2), 1: (1, 6)}  \n",
      "2139        Borda_dist  {0: (4, 1, 2), 1: (1, 6)}  \n",
      "\n",
      "[2140 rows x 7 columns]\n",
      "Index(['filename', 'n_clusters', 'outlier_fraction',\n",
      "       'positive_centers (idx, x, ballot)', 'optimal value',\n",
      "       'distance function', 'median_dict', 'medians_sorted'],\n",
      "      dtype='object')\n",
      "                         filename  n_clusters  outlier_fraction  \\\n",
      "0         aberdeen_2012_ward1.csv           2               0.0   \n",
      "1         aberdeen_2012_ward1.csv           2               0.0   \n",
      "2         aberdeen_2012_ward1.csv           2               0.0   \n",
      "3         aberdeen_2012_ward1.csv           2               0.0   \n",
      "4         aberdeen_2012_ward1.csv           2               0.0   \n",
      "...                           ...         ...               ...   \n",
      "6415  west_lothian_2022_ward9.csv           2               0.0   \n",
      "6416  west_lothian_2022_ward9.csv           2               0.0   \n",
      "6417  west_lothian_2022_ward9.csv           2               0.0   \n",
      "6418  west_lothian_2022_ward9.csv           2               0.0   \n",
      "6419  west_lothian_2022_ward9.csv           2               0.0   \n",
      "\n",
      "                positive_centers (idx, x, ballot)  optimal value  \\\n",
      "0      {(21, 1.0, (1, 3)), (627, 1.0, (5, 4, 8))}        27269.5   \n",
      "1      {(21, 1.0, (1, 3)), (627, 1.0, (5, 4, 8))}        27269.5   \n",
      "2      {(21, 1.0, (1, 3)), (627, 1.0, (5, 4, 8))}        53739.0   \n",
      "3      {(21, 1.0, (1, 3)), (627, 1.0, (5, 4, 8))}        53739.0   \n",
      "4      {(21, 1.0, (1, 3)), (627, 1.0, (5, 4, 8))}        53739.0   \n",
      "...                                           ...            ...   \n",
      "6415  {(344, 1.0, (4, 1, 2)), (585, 1.0, (1, 6))}        22865.5   \n",
      "6416  {(344, 1.0, (4, 1, 2)), (585, 1.0, (1, 6))}        44527.0   \n",
      "6417  {(344, 1.0, (4, 1, 2)), (585, 1.0, (1, 6))}        44527.0   \n",
      "6418  {(344, 1.0, (4, 1, 2)), (585, 1.0, (1, 6))}        44527.0   \n",
      "6419  {(344, 1.0, (4, 1, 2)), (585, 1.0, (1, 6))}        44527.0   \n",
      "\n",
      "     distance function                median_dict       medians_sorted  \n",
      "0              HH_dist  {0: (1, 3), 1: (5, 4, 8)}  [(1, 3), (5, 4, 8)]  \n",
      "1              HH_dist  {0: (1, 3), 1: (5, 4, 8)}  [(1, 3), (5, 4, 8)]  \n",
      "2           Borda_dist  {0: (1, 3), 1: (5, 4, 8)}  [(1, 3), (5, 4, 8)]  \n",
      "3           Borda_dist  {0: (1, 3), 1: (5, 4, 8)}  [(1, 3), (5, 4, 8)]  \n",
      "4           Borda_dist  {0: (1, 3), 1: (5, 4, 8)}  [(1, 3), (5, 4, 8)]  \n",
      "...                ...                        ...                  ...  \n",
      "6415           HH_dist  {0: (4, 1, 2), 1: (1, 6)}  [(1, 6), (4, 1, 2)]  \n",
      "6416        Borda_dist  {0: (4, 1, 2), 1: (1, 6)}  [(1, 6), (4, 1, 2)]  \n",
      "6417        Borda_dist  {0: (4, 1, 2), 1: (1, 6)}  [(1, 6), (4, 1, 2)]  \n",
      "6418        Borda_dist  {0: (4, 1, 2), 1: (1, 6)}  [(1, 6), (4, 1, 2)]  \n",
      "6419        Borda_dist  {0: (4, 1, 2), 1: (1, 6)}  [(1, 6), (4, 1, 2)]  \n",
      "\n",
      "[6420 rows x 8 columns]\n",
      "Index(['filename', 'num_cands', 'num_voters', 'num_unique_ballots',\n",
      "       'avg_ballot_len', 'ballot_lengths', 'parties', 'method', 'block_size',\n",
      "       'silB', 'silH', 'centroids_H', 'centroids_B', 'medoids_H', 'medoids_B',\n",
      "       'slates', 'medoids_Borda_sorted', 'medoids_HH_sorted'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "\n",
    "with open('Medians2.pkl', 'rb') as file:\n",
    "    optmedians = pickle.load(file)\n",
    "\n",
    "# Function to transform the original column entries\n",
    "def transform_entry(entry):\n",
    "    try:\n",
    "        # Check if the entry is a string\n",
    "        if isinstance(entry, str):\n",
    "            # Replace set braces with list brackets for parsing\n",
    "            entry = entry.replace('{', '[').replace('}', ']')\n",
    "            # Convert the string representation to an actual list of tuples\n",
    "            entry_list = ast.literal_eval(entry)\n",
    "        elif isinstance(entry, set):\n",
    "            entry_list = list(entry)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected entry type: {type(entry)}\")\n",
    "        \n",
    "        # Create the dictionary with desired format\n",
    "        result_dict = {i: tpl[2] for i, tpl in enumerate(entry_list)}\n",
    "        \n",
    "        return result_dict\n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        print(f\"Error parsing entry: {entry} - {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Apply the transformation function to the original column and create a new column\n",
    "optmedians['median_dict'] = optmedians['positive_centers (idx, x, ballot)'].apply(transform_entry)\n",
    "\n",
    "# # Ensure there's an even number of rows for pairing\n",
    "# if len(data) % 2 != 0:\n",
    "#     raise ValueError(\"DataFrame must have an even number of rows for pairing\")\n",
    "\n",
    "# # Group by every two rows and aggregate the output values\n",
    "# data_grouped = data.groupby(data.index // 2).agg({'filename': 'first', 'median_dict': lambda x: list(x)})\n",
    "\n",
    "# # Create new columns 'outputA' and 'outputB' from the aggregated output list\n",
    "# data_grouped['HH'] = data_grouped['median_dict'].apply(lambda x: x[0])\n",
    "# data_grouped['Borda'] = data_grouped['median_dict'].apply(lambda x: x[1])\n",
    "\n",
    "# # Drop the original 'output' column if no longer needed\n",
    "# data_grouped = data_grouped.drop(columns=['median_dict'])\n",
    "\n",
    "# # Optionally, reset index if you want a sequential index\n",
    "# data_grouped.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def sort_lists_in_dict(d):\n",
    "    return {k: tuple(sorted(v)) for k, v in sorted(d.items())}\n",
    "\n",
    "# Function to sort and reassign indices based on lexicographical order\n",
    "def sort_and_reassign_indices(d):\n",
    "    sorted_items = sorted(d.items(), key=lambda x: tuple(sorted(x[1])))\n",
    "    reindexed_dict = {i: v for i, (k, v) in enumerate(sorted_items)}\n",
    "    return reindexed_dict\n",
    "\n",
    "def normalize_dict(d):\n",
    "    # Sort the values (tuples) and convert to a list of tuples\n",
    "    normalized = sorted(d.values())\n",
    "    return normalized\n",
    "\n",
    "print(optmedians.columns)\n",
    "print(optmedians)\n",
    "\n",
    "\n",
    "# Apply sorting function to 'median_dict' column\n",
    "optmedians['medians_sorted'] = optmedians['median_dict'].apply(normalize_dict)\n",
    "\n",
    "# # Drop the original 'output' column if no longer needed\n",
    "# data_grouped = data_grouped.drop(columns=['HH'])\n",
    "# data_grouped = data_grouped.drop(columns=['Borda'])\n",
    "\n",
    "#print(data_grouped)\n",
    "\n",
    "# Initialize an empty list to hold the expanded rows\n",
    "expanded_rows = []\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in optmedians.iterrows():\n",
    "    if row['distance function'] == 'HH_dist':\n",
    "        # Append two identical copies of the row consecutively\n",
    "        expanded_rows.append(row)\n",
    "        expanded_rows.append(row)\n",
    "    elif row['distance function'] == 'Borda_dist':\n",
    "        # Append four identical copies of the row consecutively\n",
    "        expanded_rows.extend([row] * 4)\n",
    "    else:\n",
    "        # Handle other cases if needed\n",
    "        expanded_rows.append(row)\n",
    "\n",
    "# Create a new DataFrame from the expanded rows\n",
    "expanded_optmedians = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Reset index if needed\n",
    "expanded_optmedians.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the expanded DataFrame\n",
    "print(optmedians.columns)\n",
    "print(expanded_optmedians)\n",
    "\n",
    "output_file_path = os.path.join(os.path.expanduser('~'), 'mymedians.csv')\n",
    "expanded_optmedians.to_csv(output_file_path, index=False)\n",
    "\n",
    "\n",
    "#print(data_grouped.columns)\n",
    "\n",
    "# Function to sort dictionary entries and reassign indices\n",
    "def sort_and_reassign_indices_new(d):\n",
    "    # Sort the dictionary by the values (tuples) in lexicographical order\n",
    "    sorted_items = sorted(d.items(), key=lambda item: item[1])\n",
    "    # Create a new dictionary with sorted items and original indices\n",
    "    return {k: v for k, v in sorted_items}\n",
    "\n",
    "# # Load the data\n",
    "# with open('/mnt/data/results.pkl', 'rb') as file:\n",
    "#     hisdata = pickle.load(file)\n",
    "\n",
    "# Load the data\n",
    "file_path = '/Users/dbs10/Desktop/engri-1101/textbook/labs/k_medians_WIP/results.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    hisdata = pickle.load(file)\n",
    "\n",
    "# Filter the dataframe\n",
    "justmedoids = hisdata[hisdata['method'].isin(['medoH', 'medoBA', 'medoBC'])].copy()\n",
    "\n",
    "# Apply sorting function to 'medoids_B' column\n",
    "justmedoids.loc[:, 'medoids_Borda_sorted'] = justmedoids['medoids_B'].apply(normalize_dict)\n",
    "\n",
    "# Apply sorting function to 'medoids_H' column\n",
    "justmedoids.loc[:, 'medoids_HH_sorted'] = justmedoids['medoids_H'].apply(normalize_dict)\n",
    "\n",
    "#print(justmedoids)\n",
    "\n",
    "# with open('results.pkl', 'rb') as file:\n",
    "#     hisdata = pickle.load(file)\n",
    "\n",
    "\n",
    "# # Filter the dataframe\n",
    "# justmedoids = hisdata[hisdata['method'].isin(['medoH', 'medoBA', 'medoBC'])]\n",
    "\n",
    "\n",
    "# # Apply sorting function to 'Borda' column\n",
    "# justmedoids['medoids_Borda_sorted'] = justmedoids['medoids_B'].apply(sort_and_reassign_indices)\n",
    "# justmedoids['medoids_HH_sorted'] = justmedoids['medoids_H'].apply(sort_and_reassign_indices)\n",
    "\n",
    "# # Drop the original 'output' column if no longer needed\n",
    "# df_grouped = df_grouped.drop(columns=['medoids_H'])\n",
    "# df_grouped = df_grouped.drop(columns=['medoids_B'])\n",
    "\n",
    "# # Apply sorting function to 'Borda' column\n",
    "# df_grouped['medoids_Borda_sorted'] = df_grouped['medoids_B'].apply(sort_and_reassign_indices)\n",
    "# df_grouped['medoids_HH_sorted'] = df_grouped['medoids_H'].apply(sort_and_reassign_indices)\n",
    "\n",
    "# # Drop the original 'output' column if no longer needed\n",
    "# df_grouped = df_grouped.drop(columns=['medoids_H'])\n",
    "# df_grouped = df_grouped.drop(columns=['medoids_B'])\n",
    "\n",
    "print(justmedoids.columns)\n",
    "\n",
    "output_file_path = os.path.join(os.path.expanduser('~'), 'rawresults.csv')\n",
    "justmedoids.to_csv(output_file_path, index=False)\n",
    "\n",
    "# output_file_path = os.path.join(os.path.expanduser('~'), 'hisresults.csv')\n",
    "# df_grouped.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f39f7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3) (1, 2, 4, 6, 3, 5) 8.0\n"
     ]
    }
   ],
   "source": [
    "#setting up for Borda with L_1 metric with the full_points (or average) embedding\n",
    "election, cand_names, location = parse('data/edinburgh17-02.blt')\n",
    "#num_cands = max([item for ranking in election.keys() for item in ranking])\n",
    "\n",
    "all_ballots = list(election.keys())\n",
    "num_ballots = len(all_ballots)\n",
    "candidates = sorted(list(set([item for ranking in all_ballots for item in ranking])))\n",
    "num_cands = len(candidates)\n",
    "sample_weight = np.array([election[ballot] for ballot in all_ballots])\n",
    "\n",
    "#print(sample_weight)\n",
    "#print(num_ballots)\n",
    "#print(num_cands)\n",
    "#print(all_ballots)\n",
    "#TOLERANCE = 14\n",
    "\n",
    "# preprocess data again\n",
    "distances_dict = {}\n",
    "possible_pairs = {}\n",
    "\n",
    "    \n",
    "for i in range(len(all_ballots)):\n",
    "    temp_list = []\n",
    "    #maximum = 0\n",
    "    #total = 0\n",
    "    for j in range(len(all_ballots)):\n",
    "        distances_dict[(i,j)] = Borda_dist(all_ballots[i],all_ballots[j],num_cands, borda_style='full_points', order = 1)\n",
    "        if True:#(distances_dict[(i,j)] <= TOLERANCE): # Not sure what tolerance should be, so ignoring. If it is too slow, consider adding some tolerance.\n",
    "            temp_list.append(j)\n",
    "        #maximum = max(maximum,distances_dict[(i,j)])\n",
    "        #total = total + distances_dict[(i,j)]\n",
    "    possible_pairs[i] = temp_list\n",
    "    #print(maximum,total/len(all_ballots))\n",
    "    #print(temp_list)\n",
    "print(all_ballots[3],all_ballots[10],distances_dict[3,10])\n",
    "    \n",
    "# X = np.array([HH_proxy(ballot,num_cands=num_cands) for ballot in all_ballots])\n",
    "\n",
    "# model = KMeans(n_clusters=k, n_init=n_init).fit(X,sample_weight=sample_weight)\n",
    "# labels = model.labels_\n",
    "# centroids = model.cluster_centers_\n",
    "    \n",
    "# C = [dict() for _ in range(k)]\n",
    "# for count in range(len(all_ballots)):\n",
    "#     ballot = all_ballots[count]\n",
    "#     C[labels[count]][ballot]=election[ballot]\n",
    "# if return_centroids:\n",
    "#     return C, centroids\n",
    "# else:\n",
    "#     return C\n",
    "    \n",
    "# def Manhattan_dist(A,B):\n",
    "#     return sum(np.abs(A-B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13f7f6a7-5273-4a4d-8429-111df7aba928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(num_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2703ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter MIPGapAbs to value 0.1\n",
      "Gurobi Optimizer version 11.0.2 build v11.0.2rc0 (mac64[arm] - Darwin 23.5.0 23F79)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 1536360 rows, 1535120 columns and 6138004 nonzeros\n",
      "Model fingerprint: 0x40c0b333\n",
      "Variable types: 0 continuous, 1535120 integer (1535120 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+03]\n",
      "  Objective range  [1e+00, 3e+04]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+00]\n",
      "Presolve removed 3747 rows and 2508 columns (presolve time = 5s) ...\n",
      "Presolve removed 3747 rows and 2508 columns\n",
      "Presolve time: 7.33s\n",
      "Presolved: 1532613 rows, 1532612 columns, 4596630 nonzeros\n",
      "Variable types: 0 continuous, 1532612 integer (1532612 binary)\n",
      "Found heuristic solution: objective 118713.00000\n",
      "Deterministic concurrent LP optimizer: primal simplex, dual simplex, and barrier\n",
      "Showing barrier log only...\n",
      "\n",
      "Root barrier log...\n",
      "\n",
      "Ordering time: 0.95s\n",
      "\n",
      "Barrier statistics:\n",
      " Dense cols : 1238\n",
      " AA' NZ     : 3.065e+06\n",
      " Factor NZ  : 6.910e+06 (roughly 1.3 GB of memory)\n",
      " Factor Ops : 2.584e+09 (less than 1 second per iteration)\n",
      " Threads    : 6\n",
      "\n",
      "                  Objective                Residual\n",
      "Iter       Primal          Dual         Primal    Dual     Compl     Time\n",
      "   0   1.19861751e+08 -2.85214906e+10  7.19e+02 2.33e+03  1.70e+04    15s\n",
      "   1   5.48708169e+06 -4.07179944e+09  3.22e+01 1.12e+01  1.23e+03    16s\n",
      "   2   1.90080519e+05 -4.77984354e+07  7.95e-07 1.71e-01  1.04e+01    16s\n",
      "   3   1.80466608e+05 -6.41010524e+05  2.23e-05 2.51e-03  1.79e-01    17s\n",
      "   4   1.53495622e+05 -1.18764091e+05  6.99e-06 7.77e-04  5.92e-02    17s\n",
      "   5   1.42040889e+05  1.00621445e+04  3.67e-06 3.58e-04  2.87e-02    18s\n",
      "   6   1.38147655e+05  2.62562321e+04  2.96e-06 3.01e-04  2.43e-02    19s\n",
      "   7   1.34500120e+05  3.03111462e+04  2.43e-06 2.88e-04  2.27e-02    19s\n",
      "   8   1.31967683e+05  3.35926510e+04  2.29e-06 2.73e-04  2.14e-02   107s\n",
      "   9   1.27620029e+05  3.75369938e+04  2.06e-06 2.55e-04  1.96e-02   108s\n",
      "  10   1.22480001e+05  4.34301158e+04  1.76e-06 2.30e-04  1.72e-02   109s\n",
      "  11   1.20930733e+05  4.53875650e+04  1.67e-06 2.23e-04  1.64e-02   111s\n",
      "  12   1.19041933e+05  4.81344852e+04  1.54e-06 2.12e-04  1.54e-02   112s\n",
      "  13   1.15937343e+05  5.03335289e+04  1.33e-06 2.01e-04  1.43e-02   113s\n",
      "  14   1.14633448e+05  5.64345843e+04  1.23e-06 1.77e-04  1.27e-02   114s\n",
      "  15   1.10769039e+05  6.12101124e+04  9.73e-07 1.56e-04  1.08e-02   115s\n",
      "  16   1.10190494e+05  6.23939065e+04  9.39e-07 1.50e-04  1.04e-02   117s\n",
      "  17   1.06856882e+05  6.69981016e+04  7.69e-07 1.27e-04  8.67e-03   118s\n",
      "  18   1.04376664e+05  6.92705544e+04  6.67e-07 1.13e-04  7.64e-03   119s\n",
      "  19   1.02555255e+05  7.00845479e+04  6.02e-07 1.05e-04  7.06e-03   120s\n",
      "  20   1.01418014e+05  7.08772945e+04  5.71e-07 9.68e-05  6.64e-03   121s\n",
      "  21   9.98848370e+04  7.13711888e+04  5.36e-07 8.82e-05  6.20e-03   123s\n",
      "  22   9.76433893e+04  7.20812831e+04  4.80e-07 7.27e-05  5.56e-03   124s\n",
      "  23   9.34251892e+04  7.27724592e+04  3.86e-07 6.16e-05  4.49e-03   125s\n",
      "  24   8.71361481e+04  7.50642872e+04  2.46e-07 3.79e-07  2.63e-03   126s\n",
      "  25   7.71922343e+04  7.51727571e+04  2.75e-08 6.02e-10  4.39e-04   127s\n",
      "  26   7.57478153e+04  7.56347858e+04  1.59e-09 2.36e-10  2.46e-05   129s\n",
      "  27   7.56441038e+04  7.56439908e+04  2.44e-15 1.97e-10  2.46e-08   129s\n",
      "  28   7.56440000e+04  7.56440000e+04  2.72e-14 1.87e-10  2.98e-14   130s\n",
      "\n",
      "Barrier solved model in 28 iterations and 129.98 seconds (51.95 work units)\n",
      "Optimal objective 7.56440000e+04\n",
      "\n",
      "\n",
      "Root crossover log...\n",
      "\n",
      "    2423 DPushes remaining with DInf 0.0000000e+00               131s\n",
      "       0 DPushes remaining with DInf 0.0000000e+00               131s\n",
      "\n",
      "      50 PPushes remaining with PInf 0.0000000e+00               131s\n",
      "       0 PPushes remaining with PInf 0.0000000e+00               131s\n",
      "\n",
      "  Push phase complete: Pinf 0.0000000e+00, Dinf 0.0000000e+00    131s\n",
      "\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "    2188    7.5644000e+04   0.000000e+00   0.000000e+00    132s\n",
      "Concurrent spin time: 0.00s\n",
      "\n",
      "Solved with barrier\n",
      "\n",
      "Root relaxation: objective 7.564400e+04, 2188 iterations, 120.99 seconds (33.28 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0    75644.000000 75644.0000  0.00%     -  131s\n",
      "\n",
      "Explored 1 nodes (2188 simplex iterations) in 131.91 seconds (53.42 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 2: 75644 118713 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 7.564400000000e+04, best bound 7.564400000000e+04, gap 0.0000%\n",
      "isCenter[101] = 1.0\n",
      "isCenter[492] = 1.0\n",
      "101\n",
      "(1, 6)\n",
      "492\n",
      "(3, 5, 7)\n",
      "Optimal Objective Value: 75644.0\n"
     ]
    }
   ],
   "source": [
    "#optimizing for Borda with L_1 metric with full_points (or average) embedding\n",
    "m, pairs, isCenter, outlier = formulate_new(possible_pairs, sample_weight, distances_dict, NUM_CLUSTERS = 2)\n",
    "    # Uncomment the line below to suppress Gurobi's output\n",
    "    # m.Params.LogToConsole = 0\n",
    "m.optimize()\n",
    "\n",
    "nearest = {}\n",
    "dim = int(num_cands*(num_cands-1)/2)\n",
    "\n",
    "positive_pairs = []\n",
    "for v in m.getVars():\n",
    "    # Check if the variable name matches the pattern \"IsCenter[i]\" and if its value is positive\n",
    "    if v.varName.startswith(\"pairs[\") and v.varName.endswith(\"]\") and v.x > 0:\n",
    "        positive_pairs.append(v)\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "positive_centers = []\n",
    "for v in m.getVars():\n",
    "    # Check if the variable name matches the pattern \"IsCenter[i]\" and if its value is positive\n",
    "    if v.varName.startswith(\"isCenter[\") and v.varName.endswith(\"]\") and v.x > 0:\n",
    "        positive_centers.append(v)\n",
    "        \n",
    "for v in positive_centers:\n",
    "    print(f\"{v.varName} = {v.x}\")\n",
    "    \n",
    "for v in positive_centers:\n",
    "    next_centroid = int(v.varName[9:-1])\n",
    "    print(next_centroid)\n",
    "    print(all_ballots[next_centroid])\n",
    "\n",
    "print(f\"Optimal Objective Value: {m.objVal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee5002c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
